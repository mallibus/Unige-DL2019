{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.IntroDL_solution_Colab_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "G_LsI1CdaY2J",
        "uUrCyhLdaY2R",
        "01Ms84EmaY2W"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallibus/Unige-DL2019/blob/master/0_IntroDL_solution_Colab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2-p_EZYaY02",
        "colab_type": "text"
      },
      "source": [
        "# Lab 0. Introduction to Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2h8P6lyaY05",
        "colab_type": "text"
      },
      "source": [
        "## 0.1 Basics of TensorFlow\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/tf_logo.png\" width=\"200px\"><br>\n",
        "\n",
        "TensorFlow is an open source machine learning library for research and production.<br>\n",
        "It offers tools, libraries and resources that makes it easy for you to build and deploy ML models.\n",
        "\n",
        "### Most important features:\n",
        "* Easy model building<br>\n",
        "Build and train ML models easily using intuitive high-level APIs like Keras with eager execution, which makes for immediate model iteration and easy debugging.\n",
        "\n",
        "\n",
        "* Robust ML production anywhere<br>\n",
        "Easily train and deploy models in the cloud, on-prem, in the browser, or on-device no matter what language you use.\n",
        "\n",
        "\n",
        "* Powerful experimentation for research<br>\n",
        "A simple and flexible architecture to take new ideas from concept to code, to state-of-the-art models, and to publication faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBXV3IM5aY06",
        "colab_type": "text"
      },
      "source": [
        "### Import TensorFlow\n",
        "In this tutorial is used **version 1.13.1** that is the last stable version released.<br>\n",
        "Recently version 2.0 is released but is still a preview."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpmVVKg8aY08",
        "colab_type": "code",
        "outputId": "a9ed84a7-c390-4f5d-fe2b-9762fbdf09ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "import time, h5py, os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.14.0\n",
            "Eager execution: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2o37vByaY1D",
        "colab_type": "text"
      },
      "source": [
        "### Data representation for neural network\n",
        "A Tensor consists of a set of primitive values shaped into an array of any number of dimensions.<br>\n",
        "Similar to NumPy ndarray objects, Tensor objects have a data type and a shape.  <br>\n",
        "TensorFlow offers a rich library of operations (tf.add, tf.matmul, tf.linalg.inv etc.) that consume and produce Tensors; these operations automatically convert native Python types.<br>\n",
        "In addition, Tensors can be backed by accelerator memory (like GPU, TPU) and are immutable.<br>\n",
        "\n",
        "The rank of a tf.Tensor object defines its number of dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKbVShM1aY1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RANK 0 (scalar)\n",
        "#Create tensor of type string with value \"hello world!\" \n",
        "#Create tensor of type int16 with value 17\n",
        "#Create tensor of type float64 with value 3.14159265359\n",
        "#Create tensor of type complex64 with value 10.1 - 1.5j\n",
        "#Print all tensor\n",
        "\n",
        "t01 = tf.Variable(\"hello world!\", tf.string)\n",
        "t02 = tf.Variable(17, tf.int16)\n",
        "t03 = tf.Variable(3.14159265359, tf.float64)\n",
        "t04 = tf.Variable(10.1 - 1.5j, tf.complex64)\n",
        "print(\"RANK 0\\n\",t01,\"\\n\",t02,\"\\n\",t03,\"\\n\",t04,\"\\n\")\n",
        "\n",
        "\n",
        "#RANK 1 (vector)\n",
        "#Create tensor of type string with value \"abba\" \n",
        "#Create tensor of type float32 with value [6.14, 3.001]\n",
        "#Create tensor of type int32 with value [1,3,5,7]\n",
        "#Create tensor of type complex64 with value [10.3 - 4.05j, 3.1 - 2.13j]\n",
        "\n",
        "t11 = tf.Variable([\"abba\"], tf.string)\n",
        "t12 = tf.Variable([6.14, 3.001], tf.float32)\n",
        "t13 = tf.Variable([1, 3, 5, 7,], tf.int32)\n",
        "t14 = tf.Variable([10.3 - 4.05j, 3.1 - 2.13j], tf.complex64)\n",
        "print(\"RANK 1\\n\",t11,\"\\n\",t12,\"\\n\",t13,\"\\n\",t14,\"\\n\")\n",
        "\n",
        "\n",
        "#RANK 2 (matrix)\n",
        "#Create tensor of type int16 with values [7,4] in the first row and [11,1] in the second row \n",
        "#Create tensor of type bool with values [False,True] in the first row and [True,False] in the second row \n",
        "#Create tensor of type int32 with value [3] in the first row, [2] in the second row, [12] in the third row, [4] in the fourth row\n",
        "#Create tensor of type float64 with values [0.1,11.2,4.01,3.5] in the first row and [0.2,1,22.1,3.12] in the second row \n",
        "\n",
        "t21 = tf.Variable([[7,4],[11,1]], tf.int16)\n",
        "t22 = tf.Variable([[False, True],[True, False]], tf.bool)\n",
        "t23 = tf.Variable([[3], [2], [12], [4]], tf.int32)\n",
        "t24 = tf.Variable([[0.1,11.2,4.01,3.5],[0.2,1,22.1,3.12]], tf.float64)\n",
        "print(\"RANK 2\\n\",t21,\"\\n\",t22,\"\\n\",t23,\"\\n\",t24)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUX9ld0FaY1I",
        "colab_type": "text"
      },
      "source": [
        "### Eager execution\n",
        "TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later.<br>\n",
        "If you enable eager execution, operations like c = tf.matmul(a, b) are executed immediately.<br>\n",
        "However, without eager execution enabled, an operation like tf.matmul does not execute immediately, but, instead, builds a fragment of a TensorFlow graph (in TensorFlow 2.0, all operations will be eagerly executed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkTcoVVnaY1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.constant([[2, 8]])     \n",
        "b = tf.constant([[5],[7]])  \n",
        "c = tf.matmul(a, b)\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eJhnMqbaY1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In TensorFlow, computations can be thought of as graphs;\n",
        "# try to construct a more difficult expression  y=log(a+(b*c)^2)/(c-d)\n",
        "\n",
        "def graph(a,b,c,d):\n",
        "    prod = tf.multiply(b,c)\n",
        "    square = tf.square(prod, \"Square\")\n",
        "    arg = tf.add(a, square)\n",
        "    log = tf.log(arg) #log input types: bfloat16, half, float32, float64, complex64, complex128\n",
        "    sub = tf.subtract(c,d)\n",
        "    res = tf.divide(log,sub)\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDpcASCLaY1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.constant([[1., 2.]])     \n",
        "b = tf.constant([[5.],[7.]])\n",
        "c = tf.constant([[2., 8.]])     \n",
        "d = tf.constant([[11.],[77.]])\n",
        "\n",
        "# Execute the computation\n",
        "res = graph(a,b,c,d)\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_vj9FmKaY1S",
        "colab_type": "text"
      },
      "source": [
        "If you don't enable eager execution at the beginning of the program, you've to build a graph and run operations using a tf.Session.\n",
        "\n",
        "Example:\n",
        "\n",
        "a = tf.constant([[2, 8]])  \n",
        "b = tf.constant([[5],[7]])  \n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "with tf.Session() as sess:<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;print(sess.run(c))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0oQtkTWaY1T",
        "colab_type": "text"
      },
      "source": [
        "### Graphical Processing Unit (GPU)\n",
        "\n",
        "The basic architecture of a GPU differs a lot from a CPU; the GPU is optimized for a high computational power and a high throughput.\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/cpu_gpu.png\" width=\"500px\"><br>\n",
        "\n",
        "The computation of DNNs is a task that fits excellent on a GPU: there is a large amount of parallelism that can\n",
        "be utilized (the most common kernels are matrix multiply, convolution and functions with no data dependencies at\n",
        "all).<br>\n",
        "In TensorFlow, the supported device types are CPU and GPU; you could use also multiple-GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuOLgYQsaY1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if tf.test.is_gpu_available():\n",
        "    print(\"GPU \",tf.test.gpu_device_name(), \" is available\\n\")\n",
        "    \n",
        "# Print list of available devices\n",
        "print(\"Devices available:\\n\\n\",device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ0XksyOaY1Y",
        "colab_type": "text"
      },
      "source": [
        "### Comparison of CPU time and GPU time \n",
        "Below there's an example of the computational time required in the different cases to do matrix multiplication.<br>\n",
        "**What do you expect? Which is faster? Why?**\n",
        "\n",
        "(#We expect that computation on GPU is faster; this is due to the fact that matrix multiplication is an higly parallelizable operation and the performance is very obvious.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2J-hXUmaY1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def measure(x, steps):\n",
        "    tf.matmul(x, x)\n",
        "    start = time.time()\n",
        "    for i in range(steps):\n",
        "        x = tf.matmul(x, x)\n",
        "          # tf.matmul can return before completing the matrix multiplication\n",
        "          # (e.g., can return after enqueing the operation on a CUDA stream).\n",
        "          # The x.numpy() call below will ensure that all enqueued operations have completed \n",
        "          # (and will also copy the result to host memory, so we're including a little more than \n",
        "          # just the matmul operation time).\n",
        "    _ = x.numpy()\n",
        "    end = time.time()\n",
        "    return end - start"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZCt1zW9aY1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shape = (1000, 1000)\n",
        "steps = 200\n",
        "\n",
        "print(\"Time to multiply a {} matrix by itself {} times:\".format(shape, steps))\n",
        "\n",
        "# Run on CPU:\n",
        "with tf.device(\"/cpu:0\"):\n",
        "    print(\"CPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
        "\n",
        "# Run on GPU, if available:\n",
        "if tf.test.is_gpu_available():\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "        print(\"GPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
        "else:\n",
        "    print(\"GPU: not found\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYgCEKa9aY1g",
        "colab_type": "text"
      },
      "source": [
        "### Automatic Differentiation\n",
        "Automatic differentiation is a technique for optimizing machine learning models.<br>\n",
        "On simple terms, it is a way of automatically computing the derivatives of the output of a function using the **Chain Rule**   (https://en.wikipedia.org/wiki/Chain_rule ).<br>\n",
        "Almost every function can be computed as a composition of simple functions which have simple derivatives; consequently, you can compute the derivative of any function that can be written as composition of simpler functions.<br>\n",
        "Tensorflow uses **Reverse Mode Differentiation**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQRbMCw7aY1i",
        "colab_type": "text"
      },
      "source": [
        "#### Here you can find a quick explanation on how backpropagation work https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrP9lXUEaY1i",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow provides the tf.GradientTape API that allow to compute the gradient of a computation w.r.t. its input variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KKyB6s2aY1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the first derivative of function y=x^3 at x=2\n",
        "x = tf.constant(2.0)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "    g.watch(x)\n",
        "    y = x**3\n",
        "    \n",
        "dy_dx = g.gradient(y, x) \n",
        "print(dy_dx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grvsOuc8aY1m",
        "colab_type": "text"
      },
      "source": [
        "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called.<br>\n",
        "To compute multiple gradients over the same computation,you have to create a persistent gradient tape.<br>\n",
        "This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03TyZTeCaY1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the first and second derivative of function y=x^4 at x=3\n",
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as g:\n",
        "    g.watch(x)\n",
        "    y = x ** 2\n",
        "    z = y ** 2\n",
        "\n",
        "dz_dx = g.gradient(z, x)  # first derivative 108.0 (4*x^3 at x = 3)\n",
        "dy_dx = g.gradient(y, x)  # second derivative 6.0\n",
        "print(dz_dx,dy_dx)\n",
        "del g  # Drop the reference to the tape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_S63SmkaY1q",
        "colab_type": "text"
      },
      "source": [
        "### Keras\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/keras.png\" width=\"300px\"><br>\n",
        "\n",
        "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow.<br>\n",
        "It allows for easy and fast prototyping and supports both convolutional networks and recurrent networks.<br>\n",
        "### Most important features:\n",
        "* User friendliness\n",
        "* Modularity\n",
        "* Easy extensibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u67Q_Mv2aY1r",
        "colab_type": "text"
      },
      "source": [
        "### Build a Single Layer Perceptron\n",
        "Let's build a single layer perceptron composed by one dense layer.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/simple_nn.png\" width=\"500px\"><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G04ohiw2aY1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_dense_layer(x, n_in, n_out):\n",
        "    # n_in: number of inputs, n_out: number of outputs\n",
        "    # y = sigmoid(W*x + b)\n",
        "    # W = [1,1]\n",
        "    # b = 1\n",
        "    W = tf.ones((n_in, n_out))\n",
        "    b = tf.ones((1, n_out))\n",
        "    z = tf.matmul(x,W) + b\n",
        "    out = tf.sigmoid(z)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOpZk9fdaY1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "res = one_dense_layer(x_input, n_in=2, n_out=2)\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuNs_LWFaY1x",
        "colab_type": "text"
      },
      "source": [
        "### Build the same Single Layer Perceptron with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGyGeFxcaY1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the number of inputs and outputs\n",
        "n_input_nodes = 2\n",
        "n_output_nodes = 2\n",
        "\n",
        "# First define the model \n",
        "model = tf.keras.Sequential() # model lets us define a linear stack of network layers.\n",
        "\n",
        "# define our single fully connected network layer\n",
        "dense_layer = tf.keras.layers.Dense(n_output_nodes,activation='sigmoid',kernel_initializer=\"Ones\",bias_initializer=\"Ones\")\n",
        "\n",
        "# Add the dense layer to the model\n",
        "model.add(dense_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lst5YwLzaY11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test model\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "print(model(x_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHVo0tbuaY14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare the results obtained\n",
        "print(tf.reduce_all(tf.equal(model(x_input),one_dense_layer(x_input, n_in=2, n_out=2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O3ScCB-aY19",
        "colab_type": "text"
      },
      "source": [
        "### Build a Multilayer perceptron\n",
        "Let's build a multilayer perceptron; MLPs are fully connected, each node in one layer connects with a certain weight to every node in the following layer.\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/mlp.png\" width=\"500px\"><br>\n",
        "\n",
        "Try to build one composed by two hidden dense layer with ReLU activation and one dense output layer(units=1) with sigmoid activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8PMtoQ8aY2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate dummy data\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(2, size=(1000, 1))\n",
        "\n",
        "units = 32\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units, activation='relu', input_dim=100))\n",
        "model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, labels, epochs=30, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8fIyluGaY2E",
        "colab_type": "text"
      },
      "source": [
        "## 0.2 Build a Deep Neural Network \n",
        "*  Import the dataset\n",
        "*  Build a model\n",
        "*  Train the model \n",
        "*  Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnq7w29FaY2G",
        "colab_type": "text"
      },
      "source": [
        "### Import Fashion-MNIST Dataset\n",
        "Fashion-MNIST is a dataset of Zalando’s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. <br>\n",
        "Each example is a 28×28 grayscale image, associated with a label from 10 classes.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/fashion-mnist.png\" width=\"400px\"><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX4Gc3siaY2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_fashion = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_learn, y_learn),(x_test, y_test) = mnist_fashion.load_data()\n",
        "x_learn, x_test = x_learn / 255.0, x_test / 255.0 # normalization \n",
        "x_train, x_val, y_train, y_val = train_test_split(x_learn, y_learn, test_size=0.3, random_state=42) # split learn in train,val\n",
        "\n",
        "print(x_train.shape, x_val.shape, x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_LsI1CdaY2J",
        "colab_type": "text"
      },
      "source": [
        "#### Plot some sample from the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX44XG1RaY2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Ag6933aY2N",
        "colab_type": "text"
      },
      "source": [
        "### Build a Model\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/nn.png\" width=\"400px\"><br>\n",
        "\n",
        "#### What is a DNN?\n",
        "\n",
        "It is a neural network composed by many layers and consequently it has a deeper structure.\n",
        "\n",
        "#### How many layers?\n",
        "\n",
        "It depends on different factors: for example on the data available, on the complexity of the problem, on the computational power and so on.\n",
        "\n",
        "#### Why add non-linearity?\n",
        "<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/non-linearity.png\" width=\"700px\"><br>\n",
        "\n",
        "#### Which activation functions?\n",
        "\n",
        "There exists different choices, one of the most used is Relu but it depends on the data and on the network architecture.\n",
        "<br><br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/activation.png\" width=\"600px\"><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V53XDJeOaY2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network Parameters\n",
        "num_classes = 10 # Fashion-MNIST classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUrCyhLdaY2R",
        "colab_type": "text"
      },
      "source": [
        "#### Build a model with this structure: Flatten+Dense(ReLU)+Dense(ReLU)+Dense(ReLU)+Dense(ReLU)+Dense(softmax)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL-UXDSMaY2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/layers/core/\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXOzjck_aY2V",
        "colab_type": "text"
      },
      "source": [
        "###  Model parameters\n",
        "There are many parameters to choose from: the Optimizer, the Loss Function and the Metrics to use.<br>\n",
        "\n",
        "**Loss functions** are used to compare the network's predicted output  with the real output, in each pass of the backpropagations algorithm; loss functions are used to tell the model how the weights should be updated.<br>\n",
        "Common loss functions are: mean-squared error, cross-entropy, and so on...<br><br>\n",
        "**Metrics** are used to evaluate a model; common metrics are precision, recall, accuracy, auc,..<br>\n",
        "\n",
        "The update rules of the weights are determined by the **Optimizer**.<br>\n",
        "The performance and update speed may heavily vary from optimizer to optimizer; in choosing an optimizer what's important to consider is the network depth, the type of layers and the type of data.<br>\n",
        "The gifs below give an idea on how different Optimizers work.<br>\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/optimizer.gif\" width=\"500px\" align=\"left\">\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/optimizer1.gif\" width=\"360px\" align=\"right\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Ms84EmaY2W",
        "colab_type": "text"
      },
      "source": [
        "#### Configures the model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL2LLmKyaY2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizers    https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "sgd = tf.keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\n",
        "adad = tf.keras.optimizers.Adadelta(lr=1.0,rho=0.95,epsilon=None,decay=0.0)\n",
        "adag = tf.keras.optimizers.Adagrad(lr=0.01,epsilon=None,decay=0.0)\n",
        "adamax = tf.keras.optimizers.Adamax(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.0)\n",
        "nadam = tf.keras.optimizers.Nadam(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,schedule_decay=0.004)\n",
        "rms = tf.keras.optimizers.RMSprop(lr=0.001,rho=0.9,epsilon=None,decay=0.0)\n",
        "\n",
        "# Losses    https://keras.io/losses/\n",
        "loss = ['sparse_categorical_crossentropy','mean_squared_error','mean_absolute_error',\n",
        "        'categorical_crossentropy','categorical_hinge']\n",
        "\n",
        "# Metrics    https://www.tensorflow.org/api_docs/python/tf/metrics\n",
        "metrics = ['accuracy','precision','recall']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNQcFWhqaY2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=adam,\n",
        "              loss=loss[0],\n",
        "              metrics=[metrics[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SvR02wtaY2c",
        "colab_type": "text"
      },
      "source": [
        "### Train the model \n",
        "The batch size is a number of samples processed before the model is updated.<br>\n",
        "The number of epochs is the number of complete passes through the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd4Q9bFjaY2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 50\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, validation_data =(x_val, y_val), epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv73zOpQaY2f",
        "colab_type": "text"
      },
      "source": [
        "### Training history visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuYCcmXGaY2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "nXWlXkuBaY2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqEKGdT6aY2v",
        "colab_type": "text"
      },
      "source": [
        "**What could you notice in the loss graph training the model over large number of epochs (50 is sufficient)?**\n",
        "\n",
        "(Training loss continue to dicrease in a flatten way until to go near 0; validation loss after a while starts to increase significantly -> OVERFITTING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8zJr0nraY2w",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lxASSVmaY2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2stTI0NaY2z",
        "colab_type": "text"
      },
      "source": [
        "**Try to play with these parameters (loss and optimizers) in order to see how this choice affects the accuracy.**\n",
        "What do you expect? which is faster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znWL73IWaY20",
        "colab_type": "text"
      },
      "source": [
        "## 0.3 Overfitting\n",
        "Given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.<br>\n",
        "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether).<br>\n",
        "How to improve generalization of our model on unseen data?<br>\n",
        "There exists different methods, the most used are:\n",
        "    1. Add weight regularization\n",
        "    2. Dropout\n",
        "    3. Early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimRiLIqaY22",
        "colab_type": "text"
      },
      "source": [
        "### 0.3.1 Add weight regularization\n",
        "A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\".<br>\n",
        "This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights.<br> This cost comes in two flavors:\n",
        "* L1 regularization\n",
        "* L2 regularization\n",
        "\n",
        "In tf.keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "vE2zlBgeaY23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.001), activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.001), activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.001), activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.001), activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=adam,\n",
        "              loss=loss[0],\n",
        "              metrics=[metrics[0]])\n",
        "\n",
        "# Train\n",
        "epochs = 5\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=epochs, validation_data =(x_val, y_val))\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4xF5NBSaY25",
        "colab_type": "text"
      },
      "source": [
        "### 0.3.2 Dropout\n",
        "Dropout (http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) is one of the most effective and most commonly used regularization techniques for neural networks.<br>\n",
        "Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/dropout.png\" width=\"600px\"><br>\n",
        "\n",
        "The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5; at test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EzV7zpjJaY26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=adam,\n",
        "              loss=loss[0],\n",
        "              metrics=[metrics[0]])\n",
        "\n",
        "# Train\n",
        "epochs = 5\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=epochs, validation_data =(x_val, y_val))\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5_Yf-4UaY29",
        "colab_type": "text"
      },
      "source": [
        "### 0.3.3 Early stopping\n",
        "Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting.<br>\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/earlystopping.pbm\" width=\"400px\"><br><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcJ9tjH2aY2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# early stopping https://keras.io/callbacks/\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=5)\n",
        "\n",
        "# Create checkpoint callback that will save the best model observed during training for later use\n",
        "checkpoint_path = \"output/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,monitor='val_loss',save_weights_only=True,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oAaj7qisaY3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=adam,\n",
        "              loss=loss[0],\n",
        "              metrics=[metrics[0]])\n",
        "\n",
        "# Train\n",
        "epochs = 10\n",
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, verbose=1, callbacks=[es,cp_callback])\n",
        "# This may generate warnings related to saving the state of the optimizer.\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIT021CHaY3D",
        "colab_type": "text"
      },
      "source": [
        "### Load weights\n",
        "The saved weights can then be loaded and evaluated any time by calling the load_weights() function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0WfKJE7NaY3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This may generate warnings related to saving the state of the optimizer.\n",
        "# These warnings (and similar warnings throughout this notebook)\n",
        "# are in place to discourage outdated usage, and can be ignored.\n",
        "# link https://www.tensorflow.org/tutorials/keras/save_and_restore_models\n",
        "\n",
        "checkpoint_path = \"output/cp.ckpt\"\n",
        "\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbNy1dhPaY3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}