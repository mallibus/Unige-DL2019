{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "UNIGE DL 29019 - 0.IntroDL_Colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "RFCC8nr27Qns",
        "kGeenEt57Qoh",
        "yOhhkOEZ7Qox"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallibus/Unige-DL2019/blob/master/UNIGE_DL_29019_0_IntroDL_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFcf7Lbf7Qm1",
        "colab_type": "text"
      },
      "source": [
        "# Lab 0. Introduction to Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNeH0hmL7Qm5",
        "colab_type": "text"
      },
      "source": [
        "## 0.1 Basics of TensorFlow\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/tf_logo.png\" width=\"200px\"><br>\n",
        "TensorFlow is an open source machine learning library for research and production.<br>\n",
        "It offers tools, libraries and resources that makes it easy for you to build and deploy ML models.\n",
        "\n",
        "### Most important features:\n",
        "* Easy model building<br>\n",
        "Build and train ML models easily using intuitive high-level APIs like Keras with eager execution, which makes for immediate model iteration and easy debugging.\n",
        "\n",
        "\n",
        "* Robust ML production anywhere<br>\n",
        "Easily train and deploy models in the cloud, on-prem, in the browser, or on-device no matter what language you use.\n",
        "\n",
        "\n",
        "* Powerful experimentation for research<br>\n",
        "A simple and flexible architecture to take new ideas from concept to code, to state-of-the-art models, and to publication faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsoIMbKIoOQl",
        "colab_type": "text"
      },
      "source": [
        "### Note\n",
        "If you cannot see the image in the previous block, click on the following link. <br>\n",
        "http://mlclass.epizy.com/lab0_images_notebook/tf_logo.png\n",
        "\n",
        "You will be redirected to an image. Come back and reload this page. You should be able to see the images on this page now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRGdWdQF7Qm8",
        "colab_type": "text"
      },
      "source": [
        "### Import TensorFlow\n",
        "In this tutorial is used **version 1.13.1** that is the last stable version released.<br>\n",
        "Recently version 2.0 is released but is still a preview."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkzXJp7O7Qm-",
        "colab_type": "code",
        "outputId": "7e38783b-0bfe-4613-f837-4d3abb43af7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "import time, h5py, os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.14.0\n",
            "Eager execution: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KgI_bXk7QnD",
        "colab_type": "text"
      },
      "source": [
        "### Data representation for neural network\n",
        "A Tensor consists of a set of primitive values shaped into an array of any number of dimensions.<br>\n",
        "Similar to NumPy ndarray objects, Tensor objects have a data type and a shape.  <br>\n",
        "TensorFlow offers a rich library of operations (tf.add, tf.matmul, tf.linalg.inv etc.) that consume and produce Tensors; these operations automatically convert native Python types.<br>\n",
        "In addition, Tensors can be backed by accelerator memory (like GPU, TPU) and are immutable.<br>\n",
        "\n",
        "The rank of a tf.Tensor object defines its number of dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wd7qkcIr06Y",
        "colab_type": "code",
        "outputId": "7a29e085-d589-44e2-92a0-91feb5e518a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "tf.Variable([[0.1,11.2,4.01,3.5] , [0.2,1,22.1,3.12]], tf.float64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
              "array([[ 0.1 , 11.2 ,  4.01,  3.5 ],\n",
              "       [ 0.2 ,  1.  , 22.1 ,  3.12]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRpE40NC7QnE",
        "colab_type": "code",
        "outputId": "570aa6a1-6049-49b0-a4e4-c31cec92e330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "#RANK 0 (scalar)\n",
        "#Create tensor of type string with value \"hello world!\" \n",
        "#Create tensor of type int16 with value 17\n",
        "#Create tensor of type float64 with value 3.14159265359\n",
        "#Create tensor of type complex64 with value 10.1 - 1.5j\n",
        "\n",
        "t01 = tf.Variable(\"hello world!\", tf.string)\n",
        "t02 = tf.Variable(17, tf.int16)\n",
        "t03 = tf.Variable(3.14159265359, tf.float64)\n",
        "t04 = tf.Variable(10.1 - 1.5j, tf.complex64)\n",
        "print(\"RANK 0\\n\",t01,\"\\n\",t02,\"\\n\",t03,\"\\n\",t04,\"\\n\")\n",
        "\n",
        "\n",
        "#RANK 1 (vector)\n",
        "#Create tensor of type string with value \"abba\" \n",
        "#Create tensor of type float32 with value [6.14, 3.001]\n",
        "#Create tensor of type int32 with value [1,3,5,7]\n",
        "#Create tensor of type complex64 with value [10.3 - 4.05j, 3.1 - 2.13j]\n",
        "\n",
        "t11 = tf.Variable([\"abba\"], tf.string)\n",
        "t12 = tf.Variable([6.14, 3.001], tf.float32)\n",
        "t13 = tf.Variable([1,3,5,7], tf.int32)\n",
        "t14 = tf.Variable([10.3 - 4.05j, 3.1 - 2.13j], tf.complex64)\n",
        "print(\"RANK 1\\n\",t11,\"\\n\",t12,\"\\n\",t13,\"\\n\",t14,\"\\n\")\n",
        "\n",
        "\n",
        "#RANK 2 (matrix)\n",
        "#Create tensor of type int16 with values [7,4] in the first row and [11,1] in the second row \n",
        "#Create tensor of type bool with values [False,True] in the first row and [False,True] in the second row \n",
        "#Create tensor of type int32 with value [3] in the first row,[2] in the second row,[12] in the third row,[4] in the fourth row\n",
        "#Create tensor of type float64 with values [0.1,11.2,4.01,3.5] in the first row and [0.2,1,22.1,3.12] in the second row \n",
        "\n",
        "t21 = tf.Variable([[7,4],[11,1]], tf.int16)\n",
        "t22 = tf.Variable([[False,True],[False,True]], tf.bool)\n",
        "t23 = tf.Variable([[3],[2] ,[12] ,[4]], tf.int32)\n",
        "t24 = tf.Variable([[0.1,11.2,4.01,3.5] , [0.2,1,22.1,3.12]], tf.float64)\n",
        "print(\"RANK 2\\n\",t21,\"\\n\",t22,\"\\n\",t23,\"\\n\",t24)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RANK 0\n",
            " <tf.Variable 'Variable:0' shape=() dtype=string, numpy=b'hello world!'> \n",
            " <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=17> \n",
            " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.1415927> \n",
            " <tf.Variable 'Variable:0' shape=() dtype=complex128, numpy=(10.1-1.5j)> \n",
            "\n",
            "RANK 1\n",
            " <tf.Variable 'Variable:0' shape=(1,) dtype=string, numpy=array([b'abba'], dtype=object)> \n",
            " <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([6.14 , 3.001], dtype=float32)> \n",
            " <tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 3, 5, 7], dtype=int32)> \n",
            " <tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([10.3-4.05j,  3.1-2.13j])> \n",
            "\n",
            "RANK 2\n",
            " <tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
            "array([[ 7,  4],\n",
            "       [11,  1]], dtype=int32)> \n",
            " <tf.Variable 'Variable:0' shape=(2, 2) dtype=bool, numpy=\n",
            "array([[False,  True],\n",
            "       [False,  True]])> \n",
            " <tf.Variable 'Variable:0' shape=(4, 1) dtype=int32, numpy=\n",
            "array([[ 3],\n",
            "       [ 2],\n",
            "       [12],\n",
            "       [ 4]], dtype=int32)> \n",
            " <tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
            "array([[ 0.1 , 11.2 ,  4.01,  3.5 ],\n",
            "       [ 0.2 ,  1.  , 22.1 ,  3.12]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJfjlfXf7QnJ",
        "colab_type": "text"
      },
      "source": [
        "### Eager execution\n",
        "TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later.<br>\n",
        "If you enable eager execution, operations like c = tf.matmul(a, b) are executed immediately.<br>\n",
        "However, without eager execution enabled, an operation like tf.matmul does not execute immediately, but, instead, builds a fragment of a TensorFlow graph (in TensorFlow 2.0, all operations will be eagerly executed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC5zuNiZ7QnK",
        "colab_type": "code",
        "outputId": "147a19c3-9b0e-4f08-89b0-95825d042d6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "a = tf.constant([[2, 8]])     \n",
        "b = tf.constant([[5],[7]])  \n",
        "c = tf.matmul(a, b)\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[66]], shape=(1, 1), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-HxvtGSuTGQ",
        "colab_type": "code",
        "outputId": "d78a148c-149b-45fc-d5c1-28d467980aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "a = tf.constant([[1., 2.]])     \n",
        "b = tf.constant([[5.],[7.]])\n",
        "c = tf.constant([[2., 8.]])     \n",
        "d = tf.constant([[11.],[77.]])\n",
        "\n",
        "\n",
        "tf.math.log(a+tf.math.square(b*c))/(c-d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24571, shape=(2, 2), dtype=float32, numpy=\n",
              "array([[-0.51279116, -2.4596694 ],\n",
              "       [-0.07044271, -0.11668611]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX6Gs8re7QnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In TensorFlow, computations can be thought of as graphs;\n",
        "# try to build a more difficult expression  y=log(a+(b*c)^2)/(c-d)\n",
        "\n",
        "def graph(a,b,c,d):\n",
        "    res = y=tf.math.log(a+tf.math.square(b*c))/(c-d)\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiTfVHdM7QnT",
        "colab_type": "code",
        "outputId": "a1e42b1f-c2cf-476e-aafa-65ed72030778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "a = tf.constant([[1., 2.]])     \n",
        "b = tf.constant([[5.],[7.]])\n",
        "c = tf.constant([[2., 8.]])     \n",
        "d = tf.constant([[11.],[77.]])\n",
        "\n",
        "# Execute the computation\n",
        "res = graph(a,b,c,d)\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.51279116 -2.4596694 ]\n",
            " [-0.07044271 -0.11668611]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyEYCxlC7QnX",
        "colab_type": "text"
      },
      "source": [
        "If you don't enable eager execution at the beginning of the program, you've to build a graph and run operations using a tf.Session.\n",
        "\n",
        "Example:\n",
        "\n",
        "a = tf.constant([[2, 8]])  \n",
        "b = tf.constant([[5],[7]])  \n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "with tf.Session() as sess:<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;print(sess.run(c))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV8gwP8I7QnY",
        "colab_type": "text"
      },
      "source": [
        "### Graphical Processing Unit (GPU)\n",
        "\n",
        "The basic architecture of a GPU differs a lot from a CPU; the GPU is optimized for a high computational power and a high throughput.\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/cpu_gpu.png\" width=\"500px\"><br>\n",
        "\n",
        "The computation of DNNs is a task that fits excellent on a GPU: there is a large amount of parallelism that can\n",
        "be utilized (the most common kernels are matrix multiply, convolution and functions with no data dependencies at\n",
        "all).<br>\n",
        "In TensorFlow, the supported device types are CPU and GPU; you could use also multiple-GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIvqZ4f37QnZ",
        "colab_type": "code",
        "outputId": "1657e49b-958d-404d-e4e2-ad3ed583a462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "source": [
        "if tf.test.is_gpu_available():\n",
        "    print(\"GPU \",tf.test.gpu_device_name(), \" is available\\n\")\n",
        "    \n",
        "# Print list of available devices\n",
        "print(\"Devices available:\\n\\n\",device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU  /device:GPU:0  is available\n",
            "\n",
            "Devices available:\n",
            "\n",
            " [name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 9172689221157610775\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 16245512340323369911\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 10655429529530931490\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11326753997\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 729225830702021918\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkxGlghG7Qnd",
        "colab_type": "text"
      },
      "source": [
        "### Comparison of CPU time and GPU time \n",
        "Below there's an example of the computational time required in the different cases to do matrix multiplication.<br>\n",
        "**What do you expect? Which is faster? Why?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq2OOj0v7Qng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def measure(x, steps):\n",
        "    tf.matmul(x, x)\n",
        "    start = time.time()\n",
        "    for i in range(steps):\n",
        "        x = tf.matmul(x, x)\n",
        "          # tf.matmul can return before completing the matrix multiplication\n",
        "          # (e.g., can return after enqueing the operation on a CUDA stream).\n",
        "          # The x.numpy() call below will ensure that all enqueued operations have completed \n",
        "          # (and will also copy the result to host memory, so we're including a little more than \n",
        "          # just the matmul operation time).\n",
        "    _ = x.numpy()\n",
        "    end = time.time()\n",
        "    return end - start"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HviUhQx7Qnm",
        "colab_type": "code",
        "outputId": "06ee45c5-e274-4bf5-8eab-177525be2e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "shape = (1000, 1000)\n",
        "steps = 200\n",
        "\n",
        "print(\"Time to multiply a {} matrix by itself {} times:\".format(shape, steps))\n",
        "\n",
        "# Run on CPU:\n",
        "with tf.device(\"/cpu:0\"):\n",
        "    print(\"CPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
        "\n",
        "# Run on GPU, if available:\n",
        "if tf.test.is_gpu_available():\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "        print(\"GPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
        "else:\n",
        "    print(\"GPU: not found\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to multiply a (1000, 1000) matrix by itself 200 times:\n",
            "CPU: 6.4302473068237305 secs\n",
            "GPU: 0.2633204460144043 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7T4QGH17Qnr",
        "colab_type": "text"
      },
      "source": [
        "### Automatic Differentiation\n",
        "Automatic differentiation is a technique for optimizing machine learning models.<br>\n",
        "On simple terms, it is a way of automatically computing the derivatives of the output of a function using the **Chain Rule**   (https://en.wikipedia.org/wiki/Chain_rule ).<br>\n",
        "Almost every function can be computed as a composition of simple functions which have simple derivatives; consequently, you can compute the derivative of any function that can be written as composition of simpler functions.<br>\n",
        "Tensorflow uses **Reverse Mode Differentiation**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFCC8nr27Qns",
        "colab_type": "text"
      },
      "source": [
        "#### Here you can find a quick explanation on how backpropagation work https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDSg-A1h7Qnu",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow provides the tf.GradientTape API that allow to compute the gradient of a computation w.r.t. its input variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2T6RaVB7Qnw",
        "colab_type": "code",
        "outputId": "bcd0116e-0841-4ba5-dd9b-d1f7ac51b1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Compute the first derivative of function y=x^3 at x=2\n",
        "x = tf.constant(2.0)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "    g.watch(x)\n",
        "    y = tf.pow(x,3)\n",
        "    \n",
        "dy_dx = g.gradient(y, x) \n",
        "print(dy_dx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(12.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01K1JQrRxMqf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKdBgtvZ7Qn0",
        "colab_type": "text"
      },
      "source": [
        "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called.<br>\n",
        "To compute multiple gradients over the same computation,you have to create a persistent gradient tape.<br>\n",
        "This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28cu7bFkyvR-",
        "colab_type": "code",
        "outputId": "df03c850-a47c-4d6c-c240-671319f31bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "y = x ** 2\n",
        "y ** 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=25033, shape=(), dtype=float32, numpy=81.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0G2oKGk7Qn1",
        "colab_type": "code",
        "outputId": "05063c8f-044f-4d6e-f6a8-d0986f5d6a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Compute the first and second derivative of function y=x^4 at x=3\n",
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as g:\n",
        "    g.watch(x)\n",
        "    y = x ** 2\n",
        "    z = y ** 2\n",
        "\n",
        "dz_dx = g.gradient(z, x)  # first derivative\n",
        "dy_dx = g.gradient(y, x) # second derivative\n",
        "print(dz_dx,dy_dx)\n",
        "del g  # Drop the reference to the tape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(108.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv-BJxLy7Qn4",
        "colab_type": "text"
      },
      "source": [
        "### Keras\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/keras.png\" width=\"300px\"><br>\n",
        "\n",
        "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow.<br>\n",
        "It allows for easy and fast prototyping and supports both convolutional networks and recurrent networks.<br>\n",
        "### Most important features:\n",
        "* User friendliness\n",
        "* Modularity\n",
        "* Easy extensibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqASsWEs7Qn6",
        "colab_type": "text"
      },
      "source": [
        "### Build a Single Layer Perceptron\n",
        "Let's build a single layer perceptron composed by one dense layer.\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/simple_nn.png\" width=\"500px\"><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAYLjoij2a9_",
        "colab_type": "code",
        "outputId": "3593cbf3-922d-43ee-e345-7fa28c940668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tf.constant([[1.,2.]]) @ tf.constant([[1.],[2.]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=25118, shape=(1, 1), dtype=float32, numpy=array([[5.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-qeCsHzwsB",
        "colab_type": "code",
        "outputId": "b5e7d29a-588a-416c-b24d-f073a368d7c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "x = tf.constant([[1.,2.]])\n",
        "W = tf.constant([[1.,1.]])\n",
        "b = tf.constant(1.)\n",
        "\n",
        "tf.sigmoid(tf.tensordot(W,x,(1,1))+b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=25135, shape=(1, 1), dtype=float32, numpy=array([[0.98201376]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC8D0f5G7Qn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_dense_layer(x, n_in, n_out):\n",
        "  \n",
        "    # n_in: number of inputs, n_out: number of outputs\n",
        "    # y = sigmoid(W*x + b)\n",
        "    # W = [1,1]\n",
        "    # b = 1\n",
        "    \n",
        "  #x = tf.constant([[1.,2.]], shape=(1,2))\n",
        "  W = tf.constant([[1.,1.]])\n",
        "  b = tf.constant(1.)\n",
        "    \n",
        "  out =  tf.sigmoid(tf.tensordot(W,x,(1,1))+b)\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XoCVa627Qn_",
        "colab_type": "code",
        "outputId": "40a0fe6d-0670-413b-8e3e-f1f9123b46e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "n_in = 2\n",
        "n_out = 2\n",
        "\n",
        "res = one_dense_layer(x_input,n_in,n_out)\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.98201376]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR9CYdPZ7QoE",
        "colab_type": "text"
      },
      "source": [
        "### Build the same Single Layer Perceptron with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRjEfBqB4xL_",
        "colab_type": "code",
        "outputId": "6e7f1684-2e62-4dbb-85de-98f3e00226f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "n_input_nodes = 2\n",
        "n_output_nodes = 2\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "tf.keras.layers.Dense(n_output_nodes, activation='sigmoid', \n",
        "                      use_bias=True, kernel_initializer='ones', \n",
        "                      bias_initializer='ones',\n",
        "                      input_shape=(n_input_nodes,))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7f4a98b592e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnG_Y7aS7QoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the number of inputs and outputs\n",
        "n_input_nodes = 2\n",
        "n_output_nodes = 2\n",
        "\n",
        "# First define the model \n",
        "model = tf.keras.Sequential() # model lets us define a linear stack of network layers.\n",
        "\n",
        "# define a single fully connected network layer\n",
        "# look at https://keras.io/layers/core/ to see which parameters takes as input\n",
        "dense_layer = tf.keras.layers.Dense(n_output_nodes, \n",
        "                                    activation='sigmoid', \n",
        "                                    use_bias=True, \n",
        "                                    kernel_initializer='ones', \n",
        "                                    bias_initializer='ones',\n",
        "                                    input_shape=(n_input_nodes,))\n",
        "\n",
        "# Add the dense layer to the model using add() function\n",
        "model.add(dense_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fSteoDv7QoJ",
        "colab_type": "code",
        "outputId": "635d3a61-d3d6-4846-95cd-6838b782c970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Test model\n",
        "x_input = tf.constant([[1.,2.]], shape=(1,2))\n",
        "print(model(x_input))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.98201376 0.98201376]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V4k9SV-7QoN",
        "colab_type": "code",
        "outputId": "eeaa4acd-f7d4-4393-e62b-cd0a1bfc6698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Compare the results obtained\n",
        "print(tf.reduce_all(tf.equal(model(x_input),one_dense_layer(x_input, n_in=n_in, n_out=n_out))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(True, shape=(), dtype=bool)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BtFApPA7QoR",
        "colab_type": "text"
      },
      "source": [
        "### Build a Multilayer perceptron\n",
        "Let's build a multilayer perceptron; MLPs are fully connected, each node in one layer connects with a certain weight to every node in the following layer.\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/mlp.png\" width=\"500px\"><br>\n",
        "\n",
        "Try to build one composed by two hidden dense layer with ReLU activation and one dense output layer(units=1) with sigmoid activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyaSr2017QoT",
        "colab_type": "code",
        "outputId": "67c33b45-98b9-445d-cf31-cceb0bfa40b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Generate dummy data\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(2, size=(1000, 1))\n",
        "\n",
        "units = 32\n",
        "input_dim = 100\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units, \n",
        "                                    activation='relu', \n",
        "                                    use_bias=True, \n",
        "                                    input_shape=(input_dim,)))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units, \n",
        "                                    activation='relu', \n",
        "                                    use_bias=True))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1, \n",
        "                                    activation='sigmoid', \n",
        "                                    use_bias=True))\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, labels, epochs=30, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1000/1000 [==============================] - 0s 260us/sample - loss: 0.7034 - acc: 0.5140\n",
            "Epoch 2/30\n",
            "1000/1000 [==============================] - 0s 93us/sample - loss: 0.6923 - acc: 0.5150\n",
            "Epoch 3/30\n",
            "1000/1000 [==============================] - 0s 90us/sample - loss: 0.6863 - acc: 0.5470\n",
            "Epoch 4/30\n",
            "1000/1000 [==============================] - 0s 100us/sample - loss: 0.6832 - acc: 0.5550\n",
            "Epoch 5/30\n",
            "1000/1000 [==============================] - 0s 114us/sample - loss: 0.6809 - acc: 0.5650\n",
            "Epoch 6/30\n",
            "1000/1000 [==============================] - 0s 97us/sample - loss: 0.6725 - acc: 0.5770\n",
            "Epoch 7/30\n",
            "1000/1000 [==============================] - 0s 95us/sample - loss: 0.6704 - acc: 0.5960\n",
            "Epoch 8/30\n",
            "1000/1000 [==============================] - 0s 99us/sample - loss: 0.6669 - acc: 0.5910\n",
            "Epoch 9/30\n",
            "1000/1000 [==============================] - 0s 95us/sample - loss: 0.6619 - acc: 0.6150\n",
            "Epoch 10/30\n",
            "1000/1000 [==============================] - 0s 102us/sample - loss: 0.6588 - acc: 0.6200\n",
            "Epoch 11/30\n",
            "1000/1000 [==============================] - 0s 102us/sample - loss: 0.6515 - acc: 0.6260\n",
            "Epoch 12/30\n",
            "1000/1000 [==============================] - 0s 92us/sample - loss: 0.6451 - acc: 0.6430\n",
            "Epoch 13/30\n",
            "1000/1000 [==============================] - 0s 96us/sample - loss: 0.6421 - acc: 0.6410\n",
            "Epoch 14/30\n",
            "1000/1000 [==============================] - 0s 100us/sample - loss: 0.6332 - acc: 0.6510\n",
            "Epoch 15/30\n",
            "1000/1000 [==============================] - 0s 116us/sample - loss: 0.6282 - acc: 0.6550\n",
            "Epoch 16/30\n",
            "1000/1000 [==============================] - 0s 95us/sample - loss: 0.6237 - acc: 0.6730\n",
            "Epoch 17/30\n",
            "1000/1000 [==============================] - 0s 96us/sample - loss: 0.6160 - acc: 0.6690\n",
            "Epoch 18/30\n",
            "1000/1000 [==============================] - 0s 101us/sample - loss: 0.6006 - acc: 0.6870\n",
            "Epoch 19/30\n",
            "1000/1000 [==============================] - 0s 97us/sample - loss: 0.6043 - acc: 0.6890\n",
            "Epoch 20/30\n",
            "1000/1000 [==============================] - 0s 95us/sample - loss: 0.5922 - acc: 0.6970\n",
            "Epoch 21/30\n",
            "1000/1000 [==============================] - 0s 91us/sample - loss: 0.5874 - acc: 0.7100\n",
            "Epoch 22/30\n",
            "1000/1000 [==============================] - 0s 103us/sample - loss: 0.5762 - acc: 0.7170\n",
            "Epoch 23/30\n",
            "1000/1000 [==============================] - 0s 100us/sample - loss: 0.5637 - acc: 0.7310\n",
            "Epoch 24/30\n",
            "1000/1000 [==============================] - 0s 92us/sample - loss: 0.5626 - acc: 0.7370\n",
            "Epoch 25/30\n",
            "1000/1000 [==============================] - 0s 105us/sample - loss: 0.5561 - acc: 0.7310\n",
            "Epoch 26/30\n",
            "1000/1000 [==============================] - 0s 94us/sample - loss: 0.5448 - acc: 0.7470\n",
            "Epoch 27/30\n",
            "1000/1000 [==============================] - 0s 94us/sample - loss: 0.5398 - acc: 0.7480\n",
            "Epoch 28/30\n",
            "1000/1000 [==============================] - 0s 107us/sample - loss: 0.5256 - acc: 0.7570\n",
            "Epoch 29/30\n",
            "1000/1000 [==============================] - 0s 95us/sample - loss: 0.5221 - acc: 0.7510\n",
            "Epoch 30/30\n",
            "1000/1000 [==============================] - 0s 97us/sample - loss: 0.5106 - acc: 0.7560\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4a98871550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilE7spJuBIWd",
        "colab_type": "code",
        "outputId": "cca1d370-47f2-40f7-a538-a1a36574d406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_23 (Dense)             (None, 32)                3232      \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 4,321\n",
            "Trainable params: 4,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUOo1x6_7Qoc",
        "colab_type": "text"
      },
      "source": [
        "## 0.2 Build a Deep Neural Network \n",
        "*  Import the dataset\n",
        "*  Build a model\n",
        "*  Train the model \n",
        "*  Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrmmAxUg7Qod",
        "colab_type": "text"
      },
      "source": [
        "### Import Fashion-MNIST Dataset\n",
        "Fashion-MNIST is a dataset of Zalando’s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. <br>\n",
        "Each example is a 28×28 grayscale image, associated with a label from 10 classes.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/fashion-mnist.png\" width=\"400px\"><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb26fn5EBa7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_fashion = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_learn, y_learn),(x_test, y_test) = mnist_fashion.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6pVQ_rD7Qoe",
        "colab_type": "code",
        "outputId": "24a42bdd-4477-4019-e160-8abe019ba390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "mnist_fashion = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_learn, y_learn),(x_test, y_test) = mnist_fashion.load_data()\n",
        "\n",
        "## normalize data  \n",
        "print(x_learn.min(),x_learn.max())\n",
        "\n",
        "x_learn, x_test = x_learn/255, x_test/255\n",
        "\n",
        "print(x_learn.min(),x_learn.max()) \n",
        "\n",
        "x_train, x_val, y_train, y_val =  train_test_split( x_learn, y_learn, test_size=0.2)\n",
        "\n",
        "print(x_train.shape, x_val.shape, x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 255\n",
            "0.0 1.0\n",
            "(48000, 28, 28) (12000, 28, 28) (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGeenEt57Qoh",
        "colab_type": "text"
      },
      "source": [
        "#### Plot some sample from the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sKvHB0G7Qoj",
        "colab_type": "code",
        "outputId": "33330af4-efb7-460b-aa20-d662dec0a22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAFmCAYAAABXzZBsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x432 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqjhJ9kA7Qom",
        "colab_type": "text"
      },
      "source": [
        "### Build a Model\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/nn.png\" width=\"400px\"><br>\n",
        "\n",
        "#### What is a DNN?\n",
        "\n",
        "It is a neural network composed by many layers and consequently it has a deeper structure.\n",
        "\n",
        "#### How many layers?\n",
        "\n",
        "It depends on different factors: for example on the data available, on the complexity of the problem, on the computational power and so on.\n",
        "\n",
        "#### Why add non-linearity?\n",
        "<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/non-linearity.png\" width=\"700px\"><br>\n",
        "\n",
        "#### Which activation functions?\n",
        "\n",
        "There exists different choices, one of the most used is Relu but it depends on the data and on the network architecture.\n",
        "<br><br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/activation.png\" width=\"600px\"><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qswH9DCR7Qon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network Parameters\n",
        "num_classes = 10 # Fashion-MNIST classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU0ptXRR7Qor",
        "colab_type": "text"
      },
      "source": [
        "#### Build a model with this structure: Flatten+Dense(ReLU)+Dense(ReLU)+Dense(ReLU)+Dense(ReLU)+Dense(softmax)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2LcKru0EcgU",
        "colab_type": "code",
        "outputId": "8ee8c6b4-ab44-45f0-83a0-c41a68ede4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "\n",
        "units = 128\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
        "model.add(tf.keras.layers.Dense(units, \n",
        "                                    activation='relu', \n",
        "                                    use_bias=True))\n",
        "model.add(tf.keras.layers.Dense(units, \n",
        "                                    activation='relu', \n",
        "                                    use_bias=True))\n",
        "model.add(tf.keras.layers.Dense(units, \n",
        "                                    activation='relu', \n",
        "                                    use_bias=True))\n",
        "model.add(tf.keras.layers.Dense(num_classes, \n",
        "                                    activation='softmax', \n",
        "                                    use_bias=True))\n",
        "          \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 134,794\n",
            "Trainable params: 134,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2xzZJL77Qos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/layers/core/\n",
        "#model = # --fill here-- # "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55imnlD37Qox",
        "colab_type": "text"
      },
      "source": [
        "###  Model parameters\n",
        "There are many parameters to choose from: the Optimizer, the Loss Function and the Metrics to use.<br>\n",
        "\n",
        "**Loss functions** are used to compare the network's predicted output  with the real output, in each pass of the backpropagations algorithm; loss functions are used to tell the model how the weights should be updated.<br>\n",
        "Common loss functions are: mean-squared error, cross-entropy, and so on...<br><br>\n",
        "**Metrics** are used to evaluate a model; common metrics are precision, recall, accuracy, auc,..<br>\n",
        "\n",
        "The update rules of the weights are determined by the **Optimizer**.<br>\n",
        "The performance and update speed may heavily vary from optimizer to optimizer; in choosing an optimizer what's important to consider is the network depth, the type of layers and the type of data.<br>\n",
        "The gifs below give an idea on how different Optimizers work.<br>\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/optimizer.gif\" width=\"500px\" align=\"left\">\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/optimizer1.gif\" width=\"360px\" align=\"right\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOhhkOEZ7Qox",
        "colab_type": "text"
      },
      "source": [
        "#### Configures the model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTYnMFr87Qoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizers    https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "sgd = tf.keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\n",
        "adad = tf.keras.optimizers.Adadelta(lr=1.0,rho=0.95,epsilon=None,decay=0.0)\n",
        "adag = tf.keras.optimizers.Adagrad(lr=0.01,epsilon=None,decay=0.0)\n",
        "adamax = tf.keras.optimizers.Adamax(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.0)\n",
        "nadam = tf.keras.optimizers.Nadam(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,schedule_decay=0.004)\n",
        "rms = tf.keras.optimizers.RMSprop(lr=0.001,rho=0.9,epsilon=None,decay=0.0)\n",
        "\n",
        "# Losses    https://keras.io/losses/\n",
        "loss = ['sparse_categorical_crossentropy','mean_squared_error','mean_absolute_error',\n",
        "        'categorical_crossentropy','categorical_hinge']\n",
        "\n",
        "# Metrics    https://www.tensorflow.org/api_docs/python/tf/metrics\n",
        "metrics = ['accuracy','precision','recall']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLlKcQFf7Qo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=adam,\n",
        "              loss=loss[0],\n",
        "              metrics=[metrics[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl6DdVnH7Qo2",
        "colab_type": "text"
      },
      "source": [
        "### Train the model \n",
        "The batch size is a number of samples processed before the model is updated.<br>\n",
        "The number of epochs is the number of complete passes through the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGmiSRiH7Qo3",
        "colab_type": "code",
        "outputId": "658cbf73-a655-4c24-e52b-3827ca7dab48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 50\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, validation_data =(x_val, y_val), epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/50\n",
            "48000/48000 [==============================] - 2s 44us/sample - loss: 0.5586 - acc: 0.8027 - val_loss: 0.4163 - val_acc: 0.8492\n",
            "Epoch 2/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.3891 - acc: 0.8597 - val_loss: 0.3758 - val_acc: 0.8608\n",
            "Epoch 3/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.3465 - acc: 0.8739 - val_loss: 0.3459 - val_acc: 0.8717\n",
            "Epoch 4/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.3186 - acc: 0.8831 - val_loss: 0.3441 - val_acc: 0.8743\n",
            "Epoch 5/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.3016 - acc: 0.8898 - val_loss: 0.3504 - val_acc: 0.8763\n",
            "Epoch 6/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2879 - acc: 0.8930 - val_loss: 0.3188 - val_acc: 0.8870\n",
            "Epoch 7/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2722 - acc: 0.8991 - val_loss: 0.3191 - val_acc: 0.8855\n",
            "Epoch 8/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2610 - acc: 0.9027 - val_loss: 0.3381 - val_acc: 0.8788\n",
            "Epoch 9/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2523 - acc: 0.9053 - val_loss: 0.3037 - val_acc: 0.8921\n",
            "Epoch 10/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2412 - acc: 0.9089 - val_loss: 0.3103 - val_acc: 0.8913\n",
            "Epoch 11/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2318 - acc: 0.9133 - val_loss: 0.3092 - val_acc: 0.8876\n",
            "Epoch 12/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2238 - acc: 0.9168 - val_loss: 0.2970 - val_acc: 0.8933\n",
            "Epoch 13/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2153 - acc: 0.9175 - val_loss: 0.3105 - val_acc: 0.8922\n",
            "Epoch 14/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.2115 - acc: 0.9201 - val_loss: 0.3100 - val_acc: 0.8902\n",
            "Epoch 15/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.2051 - acc: 0.9231 - val_loss: 0.3111 - val_acc: 0.8910\n",
            "Epoch 16/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1955 - acc: 0.9258 - val_loss: 0.3085 - val_acc: 0.8956\n",
            "Epoch 17/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1883 - acc: 0.9288 - val_loss: 0.3120 - val_acc: 0.8952\n",
            "Epoch 18/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1881 - acc: 0.9293 - val_loss: 0.3177 - val_acc: 0.9001\n",
            "Epoch 19/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1776 - acc: 0.9334 - val_loss: 0.3344 - val_acc: 0.8869\n",
            "Epoch 20/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1729 - acc: 0.9345 - val_loss: 0.3298 - val_acc: 0.8961\n",
            "Epoch 21/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1677 - acc: 0.9366 - val_loss: 0.3338 - val_acc: 0.8906\n",
            "Epoch 22/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1603 - acc: 0.9387 - val_loss: 0.3598 - val_acc: 0.8906\n",
            "Epoch 23/50\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1584 - acc: 0.9404 - val_loss: 0.3398 - val_acc: 0.8975\n",
            "Epoch 24/50\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1500 - acc: 0.9425 - val_loss: 0.3584 - val_acc: 0.8953\n",
            "Epoch 25/50\n",
            "48000/48000 [==============================] - 2s 40us/sample - loss: 0.1453 - acc: 0.9447 - val_loss: 0.3438 - val_acc: 0.8953\n",
            "Epoch 26/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1438 - acc: 0.9458 - val_loss: 0.3723 - val_acc: 0.8936\n",
            "Epoch 27/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1354 - acc: 0.9491 - val_loss: 0.3798 - val_acc: 0.8988\n",
            "Epoch 28/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1329 - acc: 0.9487 - val_loss: 0.3631 - val_acc: 0.8941\n",
            "Epoch 29/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1275 - acc: 0.9523 - val_loss: 0.3959 - val_acc: 0.8936\n",
            "Epoch 30/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1261 - acc: 0.9517 - val_loss: 0.3788 - val_acc: 0.8984\n",
            "Epoch 31/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1172 - acc: 0.9554 - val_loss: 0.3724 - val_acc: 0.8980\n",
            "Epoch 32/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1209 - acc: 0.9537 - val_loss: 0.3879 - val_acc: 0.9005\n",
            "Epoch 33/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1102 - acc: 0.9576 - val_loss: 0.3992 - val_acc: 0.8940\n",
            "Epoch 34/50\n",
            "48000/48000 [==============================] - 2s 39us/sample - loss: 0.1099 - acc: 0.9586 - val_loss: 0.3965 - val_acc: 0.8983\n",
            "Epoch 35/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1034 - acc: 0.9597 - val_loss: 0.4058 - val_acc: 0.8934\n",
            "Epoch 36/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.1102 - acc: 0.9576 - val_loss: 0.4096 - val_acc: 0.8942\n",
            "Epoch 37/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.1027 - acc: 0.9608 - val_loss: 0.4222 - val_acc: 0.8923\n",
            "Epoch 38/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0952 - acc: 0.9637 - val_loss: 0.4367 - val_acc: 0.8924\n",
            "Epoch 39/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0947 - acc: 0.9646 - val_loss: 0.4780 - val_acc: 0.8907\n",
            "Epoch 40/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0960 - acc: 0.9627 - val_loss: 0.4388 - val_acc: 0.8980\n",
            "Epoch 41/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0862 - acc: 0.9668 - val_loss: 0.4450 - val_acc: 0.8999\n",
            "Epoch 42/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0863 - acc: 0.9668 - val_loss: 0.4644 - val_acc: 0.8942\n",
            "Epoch 43/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0887 - acc: 0.9663 - val_loss: 0.5051 - val_acc: 0.8954\n",
            "Epoch 44/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0837 - acc: 0.9688 - val_loss: 0.4455 - val_acc: 0.8963\n",
            "Epoch 45/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0855 - acc: 0.9674 - val_loss: 0.4498 - val_acc: 0.8939\n",
            "Epoch 46/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0756 - acc: 0.9710 - val_loss: 0.4807 - val_acc: 0.8960\n",
            "Epoch 47/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0809 - acc: 0.9688 - val_loss: 0.4778 - val_acc: 0.8932\n",
            "Epoch 48/50\n",
            "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0751 - acc: 0.9716 - val_loss: 0.4690 - val_acc: 0.9001\n",
            "Epoch 49/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0740 - acc: 0.9718 - val_loss: 0.4689 - val_acc: 0.9013\n",
            "Epoch 50/50\n",
            "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0708 - acc: 0.9732 - val_loss: 0.5011 - val_acc: 0.8948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke9Usxcz7Qo9",
        "colab_type": "text"
      },
      "source": [
        "### Training history visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KbNYuD97Qo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "mKAz7uYC7QpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPlLU9Rn7QpF",
        "colab_type": "text"
      },
      "source": [
        "**What could you notice in the loss graph training the model over large number of epochs (50 is sufficient)?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCuTJhYJ7QpH",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3hUnGK7QpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg50FGNt7QpK",
        "colab_type": "text"
      },
      "source": [
        "**Try to play with these parameters (loss and optimizers) in order to see how this choice affects the accuracy.**\n",
        "What do you expect? which is faster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgLOkB1b7QpK",
        "colab_type": "text"
      },
      "source": [
        "## 0.3 Overfitting\n",
        "Given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.<br>\n",
        "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether).<br>\n",
        "How to improve generalization of our model on unseen data?<br>\n",
        "There exists different methods, the most used are:\n",
        "    1. Add weight regularization\n",
        "    2. Dropout\n",
        "    3. Early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5YrSYB97QpL",
        "colab_type": "text"
      },
      "source": [
        "### 0.3.1 Add weight regularization\n",
        "A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\".<br>\n",
        "This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights.<br> This cost comes in two flavors:\n",
        "* L1 regularization\n",
        "* L2 regularization\n",
        "\n",
        "In tf.keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "K3vHUgVe7QpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "# hint: see kernel_regularizer parameter in the keras layers\n",
        "model = # --fill here-- #\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "# --fill here-- #\n",
        "\n",
        "\n",
        "# Train the model\n",
        "epochs = # --fill here-- # \n",
        "history = # --fill here-- # \n",
        "\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate\n",
        "_, train_acc = # --fill here-- # \n",
        "_, test_acc = # --fill here-- # \n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqyzNsp7QpO",
        "colab_type": "text"
      },
      "source": [
        "### 0.3.2 Dropout\n",
        "Dropout (http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) is one of the most effective and most commonly used regularization techniques for neural networks.<br>\n",
        "Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/dropout.png\" width=\"600px\"><br>\n",
        "The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5; at test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "N2-lMCxk7QpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "model = # --fill here-- #\n",
        "\n",
        "# Compile the model\n",
        "# --fill here-- #\n",
        "\n",
        "# Train\n",
        "epochs =  # --fill here-- # \n",
        "history = # --fill here-- # \n",
        "\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate\n",
        "_, train_acc = # --fill here-- # \n",
        "_, test_acc = # --fill here-- # \n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSHT3sky7QpT",
        "colab_type": "text"
      },
      "source": [
        "### 0.3.3 Early stopping\n",
        "Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab0_images_notebook/earlystopping.pbm\" width=\"400px\"><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgUD_S6i7QpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# early stopping https://keras.io/callbacks/\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=5)\n",
        "\n",
        "# Create checkpoint callback that will save the best model observed during training for later use\n",
        "checkpoint_path = \"output/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,monitor='val_loss',save_weights_only=True,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "1YoVdPYR7QpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "model = # --fill here-- # \n",
        "\n",
        "# Compile the model\n",
        "# --fill here-- #\n",
        "\n",
        "\n",
        "# Train\n",
        "# hint: check at the callbacks parameter of the fit() function\n",
        "epochs = # --fill here-- # \n",
        "history = # --fill here-- # \n",
        "\n",
        "# This may generate warnings related to saving the state of the optimizer.\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate\n",
        "_, train_acc = # --fill here-- # \n",
        "_, test_acc = # --fill here-- # \n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW-zgM3s7Qpi",
        "colab_type": "text"
      },
      "source": [
        "### Load weights\n",
        "The saved weights can then be loaded and evaluated any time by calling the load_weights() function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DTHkE6x47Qpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This may generate warnings related to saving the state of the optimizer.\n",
        "# These warnings (and similar warnings throughout this notebook)\n",
        "# are in place to discourage outdated usage, and can be ignored.\n",
        "# link https://www.tensorflow.org/tutorials/keras/save_and_restore_models\n",
        "\n",
        "checkpoint_path = \"output/cp.ckpt\"\n",
        "\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}