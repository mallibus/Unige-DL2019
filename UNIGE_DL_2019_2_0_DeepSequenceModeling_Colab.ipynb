{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "UNIGE DL 2019 - 2.0.DeepSequenceModeling_Colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "AtE9OW9Z62MY",
        "c0UIqKXc62M6"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallibus/Unige-DL2019/blob/master/UNIGE_DL_2019_2_0_DeepSequenceModeling_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxgncF8D62LU",
        "colab_type": "text"
      },
      "source": [
        "# Lab 2. Deep Sequence Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fysLMkg62LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import os, json, re \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JZvjRpg62LX",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Deal with sequential data\n",
        "In this lab we see Deep Learning models that can process sequential data (text, timeseries,..).<br>\n",
        "These models don’t take as input raw text: they only work with numeric tensors; **vectorizing** text is the process of transforming text into numeric tensors.<br><br><br>\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/vectorizing.png\" width=\"400px\"><br><br><br>\n",
        "The different units into which you can break down text (words, characters) are called tokens; then if you apply a tokenization scheme, you associate numeric vectors with the generated tokens.<br>\n",
        "These vectors, packed into sequence tensors, are fed into Deep Neural Network.<br>\n",
        "There are multiple ways to associate a vector with a token: we will see One-Hot Encoding and Token Embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2dicI4v62LY",
        "colab_type": "text"
      },
      "source": [
        "### 1) One-Hot Encoding\n",
        "One-Hot Encoding consists of associating a unique integer index with every word and then turning this integer index $i$ into a binary vector of size $N$ (the size of the vocabulary); the vector is all zeros except for the $i$-th entry, which is 1.\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/one-hot.png\" width=\" 400px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p9KPML662LZ",
        "colab_type": "text"
      },
      "source": [
        "#### Try to perform One-Hot Encoding using Tokenizer\n",
        "Keras provides the Tokenizer class for preparing text documents for DL.<br>\n",
        "The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC4rylOz62LZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define 4 documents\n",
        "docs = ['Well done!','Good work','Great effort','nice work']\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# fit the tokenizer on the documents\n",
        "# --fill here-- # use fit_on_texts() function\n",
        "\n",
        "\n",
        "encoded_docs = # --fill here-- # use the function texts_to_matrix()\n",
        "print(encoded_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGXUpApj62Lb",
        "colab_type": "text"
      },
      "source": [
        "Some problems related to this kind of encoding are sparsity of the solution and the high dimensionality of the vector encoding of the tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmprxdyN62Lc",
        "colab_type": "text"
      },
      "source": [
        "### 2) Word embedding\n",
        "The vector obtained from word embedding is dense and has lower dimensionality w.r.t One-Hot Encoding vector; the dimensionality of embedding space vector is an hyperparameter.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/one-hot-we.png\" width=\"400px\"><br>\n",
        "There are two ways to obtain word embeddings:<br>\n",
        "* May be learned jointly with the network\n",
        "* May use pre-trained word vectors (Word2Vec, GloVe,..)\n",
        "\n",
        "\n",
        "Word embeddings maps human language into a geometric space; in a reasonable embedding space synonyms are embedded into similar word vectors and the geometric distance between any two word vectors reflects the semantic distance between the associated words (words meaning different things are embedded at points far away from each other, whereas related words are closer).<br>\n",
        "How good is a word-embedding space depends on the specific task.<br>\n",
        "It is reasonable to learn a new embedding space with every new task: with backpropagation and Keras it reduces to learn the weights of the Embedding layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71joK1zJ62Lc",
        "colab_type": "text"
      },
      "source": [
        "### Learning Word Embeddings with the embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMqMfgON62Ld",
        "colab_type": "text"
      },
      "source": [
        "#### Load imdb dataset\n",
        "This dataset contains movies reviews from IMDB, labeled by sentiment(positive/negative); reviews have been preprocessed, and each review is encoded as a sequence of word indexes(integers).<br>\n",
        "https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx-5LxaI62Le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "imdb = tf.keras.datasets.imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "x_train = # --fill here-- # use preprocessing.sequence.pad_sequences\n",
        "x_test = # --fill here-- # use preprocessing.sequence.pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zELULFYv62Lg",
        "colab_type": "text"
      },
      "source": [
        "#### Show the size of vocabulary and the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQa0QOvg62Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index = imdb.get_word_index()\n",
        "\n",
        "vocab_size = # --fill here-- # \n",
        "print('Vocab size : ', vocab_size)\n",
        "\n",
        "\n",
        "words_freq_list = []\n",
        "for (k,v) in imdb.get_word_index().items():\n",
        "    # --fill here-- #\n",
        "\n",
        "sorted_list = sorted(words_freq_list, key=lambda x: x[1])\n",
        "\n",
        "print(\"50 most common words: \\n\")\n",
        "print(sorted_list[0:50])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5srmJK2P62Li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index['otherwise']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79mL88aA62Lk",
        "colab_type": "text"
      },
      "source": [
        "#### Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ4oweHO62Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(10000, 8, input_length=maxlen))   \n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddNe-vvd62Ln",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJd_8mQ962Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- # you can use rms as optimizer and binary crossentropy as loss function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lTiojCK62Lr",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7HG2GYc62Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfEFx2y962Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = # --fill here-- # "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsbuPSlF62Lw",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5vtqdoL8gQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PARUEpcD62Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOlE_Fc862Ly",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGzamwGn62Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = # --fill here-- # \n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDn4sKAl62L0",
        "colab_type": "text"
      },
      "source": [
        "The Dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word\n",
        "relationships and sentence structure (for example, this model would likely treat both “this movie is a bomb” and “this movie is the bomb” as being negative reviews). It’s much better to add recurrent layers on top of the embedded sequences to learn features that take into account each sequence as a whole."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYlNWy5462L1",
        "colab_type": "text"
      },
      "source": [
        "### Using pre-trained Word Embeddings\n",
        "If you have little training data available and you can’t use your data alone to learn an appropriate task-specific embedding of your vocabulary, you can load embedding vectors from a precomputed embedding space thath exhibits useful properties that  captures generic aspects of language structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc_cVvkP62L1",
        "colab_type": "text"
      },
      "source": [
        "#### Parsing the GloVe word-embeddings file\n",
        "You can find the Glove word-embeddings file here http://nlp.stanford.edu/data/glove.6B.zip or here https://drive.google.com/drive/folders/1wvyeiRwYAdypLfrOfIaiwBMPPTzQwKp_ you can find the .txt file already extracted.<br>\n",
        "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnDYaDPCGuCx",
        "colab_type": "text"
      },
      "source": [
        "**Get path to file saved at your Google Drive folder (same procedure as lab 1)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1KFz0TTotkU",
        "colab_type": "text"
      },
      "source": [
        "Load one example of image from a Google Drive folder\n",
        "* Click on arrow at left side of screen, then \"Files\". On the Directory Tree, navigate to \"gdrive\", which will contain your Drive folder as \"My Drive\"\n",
        "* In My Drive, search for the folder containing this Lab-2 and find \"glove.6B”\n",
        "* right click on that corresponding folder, “copy path” \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmqrp9k362L2",
        "colab_type": "code",
        "outputId": "62362e00-b726-49e0-f73b-ede8f62c2cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# THE SAME PROCEDURE DONE FOR LAB1 TO LOAD IMAGES FROM A GOOGLE DRIVE FOLDER \n",
        "# HAS TO BE DONE HERE \n",
        "glove_dir = # --fill here-- # path where you save 'glove.6B.100d.txt'\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\") # for windows encoding \"utf8\" works; for linux/ios check\n",
        "\n",
        "# Parse the .txt file to build an index that maps words (as strings) to their vector representation (as number vectors).\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "    \n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-fa83aaec9041>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    glove_dir = # --fill here-- # path where you save 'glove.6B.100d.txt'\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCqETLzM62L4",
        "colab_type": "text"
      },
      "source": [
        "#### Load imdb dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjUBgEUn62L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "imdb = tf.keras.datasets.imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "x_train = # --fill here-- # use preprocessing.sequence.pad_sequences\n",
        "x_test = # --fill here-- # use preprocessing.sequence.pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LxUeJi362L7",
        "colab_type": "text"
      },
      "source": [
        "#### Preparing the GloVe word-embeddings matrix\n",
        "Now we build an embedding matrix that you can load into an Embedding layer.<br>\n",
        "Each entry contains the embedding_dim-dimensional vector for the word of the index in the reference word index (built during tokenization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJAN35L462L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimensionality of word embeddings\n",
        "embedding_dim = 100\n",
        "\n",
        "# Word from this index are valid words. i.e  3 -> 'the' which is the most frequent word\n",
        "index_from = 3\n",
        "\n",
        "word_to_index = {k:(v+index_from-1) for k,v in imdb.get_word_index().items()}\n",
        "word_to_index[\"<PAD>\"] = 0\n",
        "word_to_index[\"<START>\"] = 1\n",
        "word_to_index[\"<UNK>\"] = 2\n",
        "\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size+index_from, embedding_dim))\n",
        "\n",
        "# unknown words are mapped to zero vector\n",
        "embedding_matrix[0] = np.array(embedding_dim*[0])\n",
        "embedding_matrix[1] = np.array(embedding_dim*[0])\n",
        "embedding_matrix[2] = np.array(embedding_dim*[0])\n",
        "\n",
        "for word, i in word_to_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = # --fill here-- # \n",
        "    #else :\n",
        "        #print(word, ' not found in GLoVe file.')\n",
        "\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "print('Coverage = ', nonzero_elements / vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIi3k3dr62L-",
        "colab_type": "text"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAPf4KT562MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size+index_from, embedding_dim, input_length=maxlen))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZLcKVor62MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyvSxJV62MK",
        "colab_type": "text"
      },
      "source": [
        "#### Loading pretrained word embeddings into the Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy7nugqr62MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- # look at method tf.keras.layers.set_weights and at property tf.keras.layers.trainable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-ZwWiyG62MM",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDYHkdMP62MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF669_z_62MP",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3M2CutK62MP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKj9Cq3b62MR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPtu-mnW62MS",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIUqGX_E62MS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = # --fill here-- #\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNGSziSf62MU",
        "colab_type": "text"
      },
      "source": [
        "You can notice that using few training samples, the performance is poor.<br>\n",
        "If you try to train the same model without loading the pretrained word embeddings and without freezing the embedding layer, you’ll learn a task specific embedding of the input tokens, which is generally more powerful than pretrained word embeddings when lots of data is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2xHz1st62MU",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Recurrent Neural Network\n",
        "Here https://colah.github.io/posts/2015-08-Understanding-LSTMs/ you can find a clear explanation about RNNs and LSTMs; the following is a summary of the main concepts.\n",
        "\n",
        "\n",
        "A major characteristic of some neural networks, as ConvNet, is that they have no memory: each input is processed independently, with no state kept in between inputs.<br>\n",
        "With such networks, in order to process a sequence or a temporal series of data points, you have to show the entire sequence to the network at once (turn it into a single data point).<br>\n",
        "Biological intelligence processes information incrementally while maintaining an internal model of what it’s processing, built from past information and constantly updated as new information comes in.<br>\n",
        "A recurrent neural network (RNN) adopts the same principle but in an extremely simplified version: it processes sequences by iterating through the sequence elements and maintaining a state containing information relative to what it has seen so far. In effect, an RNN is a type of neural network that has an internal loop.\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/rnn.png\" width=\"650px\"><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Each input $x_{i=t-1, t, t+1, ..}$ is combined with the internal state and then is applied an activation function (e.g. $tanh$); then the output is computed $h_{i=t-1, t, t+1, ..}$ and the internal state is updated.<br>\n",
        "In many cases, you just need the last output ($h_{i=last t}$ at the end of the loop), because it already contains information\n",
        "about the entire sequence.\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/rnn2.png\" width=\"550px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYDqQo1Y62MV",
        "colab_type": "text"
      },
      "source": [
        "#### Numpy implementation of RNN\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/rnn1.png\" width=\"550px\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2F2W4Wq62MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timesteps = 100\n",
        "input_features = 32\n",
        "output_features = 64\n",
        "\n",
        "inputs = np.random.random((timesteps, input_features))\n",
        "\n",
        "state_t = # --fill here-- #  initial state all 0s\n",
        "\n",
        "# set W,u and b to random values\n",
        "W = # --fill here-- #\n",
        "U = # --fill here-- #\n",
        "b = # --fill here-- #\n",
        "\n",
        "successive_outputs = []\n",
        "\n",
        "for input_t in inputs:\n",
        "    output_t = # --fill here-- # \n",
        "    successive_outputs.append(output_t) \n",
        "    state_t = # --fill here-- # \n",
        "    \n",
        "final_output_sequence = np.concatenate(successive_outputs, axis=0)  # The final output is a 2D tensor of \n",
        "                                                                    # shape (timesteps, output_features).\n",
        "final_output_sequence[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLFB1hHw62MY",
        "colab_type": "text"
      },
      "source": [
        "In this example, the final output is a 2D tensor of shape (timesteps, output_features), where each timestep is the output of the loop at time t (so it contains information about all timesteps); as has already been said, in the majority of the cases you just need the last output (output_t at the end of the loop)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtE9OW9Z62MY",
        "colab_type": "text"
      },
      "source": [
        "#### RNN with tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKY4pr0K62MZ",
        "colab_type": "text"
      },
      "source": [
        "#### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EjCfQD362MZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "imdb = tf.keras.datasets.imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# --fill here-- # use preprocessing.sequence.pad_sequences\n",
        "x_train = # --fill here-- #\n",
        "x_test = # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftG9eqp762Mb",
        "colab_type": "text"
      },
      "source": [
        "#### Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zapZsps62Mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(10000, 32))\n",
        "model.add(tf.keras.layers.SimpleRNN(32))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-91Uj862Md",
        "colab_type": "text"
      },
      "source": [
        "#### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-mouXC962Md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vMPmK5v62Mf",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ty634G62Mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkq7R8IH62Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTlxrgET62Mm",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOtQgzv62Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = # --fill here-- #\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b38oJfRB62Mo",
        "colab_type": "text"
      },
      "source": [
        "#### Try to stack several recurrent layers one after the other in order to increase the representational power of a network. In such a setup, you have to get all of the intermediate layers to return full sequence of outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D6B1QfY62Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLVFdMk462Mq",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddP2qBJb62Mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbi4fuy762Ms",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61HKXIia62Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4x2JzRG62Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KxSJej662M3",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUCrE3X162M3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = # --fill here-- #\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKOA0EQt62M6",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 LSTM Network\n",
        "LSTMs are a special kind of recurrent neural network which works, for many tasks, much better than the standard RNNs.<br>\n",
        "These nets are capable of learning long-term dependencies (they are explicitly designed to avoid the long-term dependency problem); remembering information for long periods of time is practically their default behavior.<br><br>\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/lstm.png\" width=\"650px\"><br>\n",
        "\n",
        "RNNs have a very simple structure, such as a single $tanh$ layer.<br>\n",
        "LSTMs also have a chain like structure, but the repeating module has a different structure: instead of having a single neural network layer, there are four, interacting in a very special way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0UIqKXc62M6",
        "colab_type": "text"
      },
      "source": [
        "#### LSTM Walk Through\n",
        "\n",
        "The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.<br>\n",
        "It runs straight down the entire chain, with only some minor linear interactions.\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/lstm_cellstate.png\" width=\"650px\"><br>\n",
        "The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.<br>\n",
        "Gates are a way to optionally let information through; they are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\n",
        "\n",
        "The sigmoid layer outputs numbers in $[0,1]$, describing how much of each component should be let through ($0$ means “let nothing through,” $1$ means “let everything through”); an LSTM has three of these gates, to protect and control the cell state.\n",
        "\n",
        "#### How LSTM works?\n",
        "\n",
        "**1.** The first step is to decide what information we’re going to throw away from the cell state; this decision is made by a sigmoid layer called the “forget gate layer.” \n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/lstm_fg.png\" width=\"650px\"><br>\n",
        "\n",
        "**2.** The next step is to decide what new information we’re going to store in the cell state. \n",
        "First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, $\\tilde{C}$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/lstm_ig.png\" width=\"650px\"><br>\n",
        "\n",
        "**3.** Now we update the old cell state, $C_{t−1}$, into the new cell state $C_{t}$.<br>\n",
        "We multiply the old state by $f_{t}$, forgetting the things we decided to forget earlier; then we add $i_{t}$∗$\\tilde{C_{t}}$. This is the new candidate values, scaled by how much we decided to update each state value.\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/lstm_up.png\" width=\"650px\"><br>\n",
        "\n",
        "**4.** Finally the output will be based on our cell state, but will be a filtered version. \n",
        "First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values in $[−1,1]$) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
        "\n",
        "<img src=\"http://mlclass.epizy.com/lab2_images_notebook/lstm_out.png\" width=\"650px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyIQ4JIc62M6",
        "colab_type": "text"
      },
      "source": [
        "#### Create LSTM model in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNSxkIh962M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(20000, 128))\n",
        "model.add(tf.keras.layers.CuDNNLSTM(128))# if no GPU tf.keras.layers.LSTM \n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pChXMl9162M8",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-0OmGLd62M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbuGZ7Oq62M_",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWV60jel62M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWhnwacs62NC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMY01wHf62NE",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG5DwDMI62NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = # --fill here-- #\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re980O_262NF",
        "colab_type": "text"
      },
      "source": [
        "**The dataset is actually too small for LSTM to be of any advantage compared to simpler models.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdA00fzT62NG",
        "colab_type": "text"
      },
      "source": [
        "### BONUS - Try to implement a model for the yelp review dataset\n",
        "You can download the dataset in .json format from link: https://www.yelp.com/dataset/download\n",
        "\n",
        "**This file has a size of about 8GB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOcA9GpN62NG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(x):\n",
        "    \"\"\" This function convert the .json into a dict with all information about the review\n",
        "        e.g. 'review_id': 'Q1sbwvVQXV2734tPgoKj4Q', 'user_id': 'hG7b0MtEbXx5QzbzE6C_VA',..\"\"\"\n",
        "    ob = json.loads(x)\n",
        "    for k, v in ob.items():\n",
        "        if isinstance(v, list):\n",
        "            ob[k] = ','.join(v)\n",
        "        elif isinstance(v, dict):\n",
        "            for kk, vv in v.items():\n",
        "                ob['%s_%s' % (k, kk)] = vv\n",
        "            del ob[k]\n",
        "    return ob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57z81KNV62NJ",
        "colab_type": "text"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uBpQ_Db62NN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_filename = # --fill here-- # path where review.json is located\n",
        "\n",
        "with open(json_filename,'rb') as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "print(len(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj8EhWPA62NO",
        "colab_type": "text"
      },
      "source": [
        "#### Using all dataset could crush your laptop memory; in order to avoid it, we can use only a part of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAZc7OQ762NP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind = int(len(data)/10)\n",
        "data1 = data[:ind]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNd5aC3w62NQ",
        "colab_type": "text"
      },
      "source": [
        "#### Now store data in a pandas DataFrame converting each review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5BKegnP62NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame([convert(line) for line in data1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkCslaYJ62NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del(data)\n",
        "del(data1)\n",
        "data = df[['text', 'stars']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-YL5Os362NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZOTM04_62NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rates above 3 are considered positives\n",
        "data['sentiment'] = ['pos' if (x>3) else 'neg' for x in data['stars']]\n",
        "\n",
        "data['text']= [x.lower() for x in data['text']]\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2SdEpsp62NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx,row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG_6Yun062Na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth',-1)\n",
        "data[:5]\n",
        "\n",
        "data.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OWwykxA62Nb",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NG8xt5r62Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57d1c27b62Nd",
        "colab_type": "text"
      },
      "source": [
        "#### Vectorizing the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz-vC0mS62Nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=nb_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                                   lower=True,split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bak_0a5162Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hhPl6xR62Ng",
        "colab_type": "text"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcYpBZzE62Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_dim = 128\n",
        "\n",
        "model = # --fill here-- # \n",
        "# hint you could stack and embedding layer followed by an RNN or LSTM \n",
        "# the dropout and finally a dense with softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtw0Lzo_62Nk",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR8HEBtw62Nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MmJck4a62Nm",
        "colab_type": "text"
      },
      "source": [
        "#### Split data into Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82IsaC_W62Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert categorical variable into dummy variables\n",
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "x_train, x_test, y_train, y_test = # --fill here-- #\n",
        "\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_b3O4Iy62No",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKIjIGGJ62No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "history = # --fill here-- # training could be very time consuming, so set epochs variable in an appropriate way"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmMg7RVF62Nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEQFXMww62Ns",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMGv2TzR62Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = # --fill here-- #\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}