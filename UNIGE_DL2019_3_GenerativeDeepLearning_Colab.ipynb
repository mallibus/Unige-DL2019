{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "UNIGE DL2019 -  3.GenerativeDeepLearning_Colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallibus/Unige-DL2019/blob/master/UNIGE_DL2019_3_GenerativeDeepLearning_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0OtTft3dDUF",
        "colab_type": "text"
      },
      "source": [
        "# Lab 3. GenerativeDeepLearning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhef3s2HdDUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "from scipy.stats import norm\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIrN8vB-dDUK",
        "colab_type": "text"
      },
      "source": [
        "# 3. Generative Deep Learning\n",
        "Until now we have seen model that works with labeled data; now we move in an unsupervised setting: we have data $X$ without labels, and the goal is to learn some hidden or underlying structure of the data (e.g. in ML dimensionality reduction, ..).<br>\n",
        "More specifically, the goal of generative models is to take as input training samples from some distribution and learn a model that represents that distribution; once we have that model, we can use it to generate new data.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab3_images_notebook/sample.png\" width=450px><br>\n",
        "We want to learn $P_{model}(x)$ similar to $P_{data}(x)$.\n",
        "\n",
        "We see three classes of models:\n",
        "    1. Autoencoders \n",
        "    2. Variational Autoencoders (VAEs)\n",
        "    3. Generative Adversarial Networks (GANs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SqzOP7wdDUL",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Autoencoders\n",
        "An autoencoder is a neural network that is trained to attempt to copy its input to its output.<br>The network may be viewed as consisting of two parts: an encoder function $h = f(x)$ and a decoder that produces a reconstruction $r = g(h)$.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab3_images_notebook/autoencoder.jpg\" width=180px><br>\n",
        "Traditionally, autoencoders were used for dimensionality reduction or feature learning; recently, theoretical connections with latent variable models have brought autoencoders to the forefront of generative modeling.<br>\n",
        "\n",
        "Autoencoders learn a “compressed representation” of input automatically by first compressing the input (encoder) and decompressing it back (decoder) to match the original input.<br>\n",
        "The learning is aided by using distance function that quantifies the information loss that occurs from the lossy compression.<br>\n",
        "<img src=\"http://mlclass.epizy.com/lab3_images_notebook/autoenc.jpg\" width=650px><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNlgEM4mdDUM",
        "colab_type": "text"
      },
      "source": [
        "### Build a simple autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XlffPk7dDUM",
        "colab_type": "text"
      },
      "source": [
        "#### Load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajCu3fIsdDUN",
        "colab_type": "code",
        "outputId": "48f46edc-177e-48d4-d83e-03d31db41430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "(x_train, _), (x_test, _) = # --fill here-- #\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train  = # --fill here-- # reshape data (from 60000*28*28 to 60000*784)\n",
        "x_test =  # --fill here-- # reshape data (from 60000*28*28 to 60000*784)\n",
        "\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d9a3d5eda8fd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (x_train, _), (x_test, _) = # --fill here-- #\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycbqk9u4dDUQ",
        "colab_type": "text"
      },
      "source": [
        "#### Build a simple autoencoder\n",
        "Until now we have built model by creating a sequential layer and then adding other layers.<br>\n",
        "Another way to do it is to declare each layer specifing the previous layer (that produces the input for the current layer) and finally build the model:<br>\n",
        "\n",
        "        e.g.  input_layer = tf.keras.layers.Input(shape=(n,))\n",
        "              layer1 = tf.keras.layers.Dense(m)(input_layer)\n",
        "              layer2 = tf.keras.layers.Dense(m)(layer1)\n",
        "              model = tf.keras.models.Model(input_layer, layer2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3a-8OjXdDUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is the size of our encoded representations\n",
        "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
        "\n",
        "# this is our input layer\n",
        "input_img = tf.keras.layers.Input(shape=(x_train.shape[1],))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_img)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = tf.keras.layers.Dense(x_train.shape[1], activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = # --fill here-- #\n",
        "\n",
        "\n",
        "# this model maps an input to its encoded representation\n",
        "encoder = tf.keras.models.Model(input_img, encoded)\n",
        "\n",
        "# create a input layer for an encoded (32-dimensional) input to be used by decoder\n",
        "encoded_input = tf.keras.layers.Input(shape=(encoding_dim,))\n",
        "\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = # --fill here-- # \n",
        "\n",
        "# create the decoder model\n",
        "decoder = tf.keras.models.Model(encoded_input, decoder_layer(encoded_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_UZ_4wkdDUT",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmwzWBqudDUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "autoencoder.compile(optimizer=adam, loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGkUAXwsdDUX",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gwamaPa2dDUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = autoencoder.fit(x_train, x_train, \n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87MiIEQAdDUa",
        "colab_type": "text"
      },
      "source": [
        "#### Plot training & validation loss values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UglepiudDUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_hist(history):\n",
        "    # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke8abSSvdDUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_hist(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX2w_5LcdDUh",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize some reconstructed images (outputs of the decoder $r = g(h)$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KFpXJS1dDUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)\n",
        "\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(1,n+1):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f740jQXYdDUk",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize some outputs of the encoder $h = f(x)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyKXnh-fdDUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 8))\n",
        "for i in range(1,n+1):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(encoded_imgs[i].reshape(4, 8).T)\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATirGxhadDUo",
        "colab_type": "text"
      },
      "source": [
        "#### Try to use a regularizer (as L1) in the encoder. How does this affect the results? What can you observe?\n",
        "#Induce sparsity, less validation loss value, reconstructed images less accurate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Qa_qxZEKdDUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #\n",
        "# hint: Let's train this model for 100 epochs (with the added regularization the model is less likely to overfit and can be \n",
        "# trained longer)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67iQNnMxdDUq",
        "colab_type": "text"
      },
      "source": [
        "### Build a deeper autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_smKfYscdDUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = # --fill here-- #\n",
        "encoded = # --fill here-- #\n",
        "encoded = # --fill here-- #\n",
        "encoded = # --fill here-- # units = encoding_dim\n",
        "\n",
        "decoded = # --fill here-- #\n",
        "decoded = # --fill here-- #\n",
        "decoded = # --fill here-- #\n",
        "\n",
        "autoencoder = # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra2I3xQkdDUs",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO2mtbl4dDUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPo9wBX4dDUv",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Hzs7PvEXdDUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- # think about the epochs required"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYW1OzPhdDUy",
        "colab_type": "text"
      },
      "source": [
        "#### Plot training & validation loss values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6uE6WifdDUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxoRSN-ydDU1",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize some reconstructed imagges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRBwQUhvdDU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_X7WC2hdDU5",
        "colab_type": "text"
      },
      "source": [
        "### Build a Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dGaFd0PdDU8",
        "colab_type": "text"
      },
      "source": [
        "#### Load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG8mTFfhdDU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, _), (x_test, _) = # --fill here-- #\n",
        "\n",
        "x_train = # --fill here-- # normalize\n",
        "x_test = # --fill here-- # normalize\n",
        "\n",
        "\n",
        "x_train = # --fill here-- # reshape (60000,28,28)->(60000,28,28,1)\n",
        "x_test = # --fill here-- # reshape (60000,28,28)->(60000,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zb9MlFmdDVB",
        "colab_type": "text"
      },
      "source": [
        "#### Build a convolutional autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfZpCZCndDVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1))  \n",
        "\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = # --fill here-- # Conv2D\n",
        "x = # --fill here-- # MaxPooling\n",
        "x = # --fill here-- # Conv2D\n",
        "encoded = # --fill here-- # MaxPooling\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = # --fill here-- # Conv2D\n",
        "x = # --fill here-- # UpSampling\n",
        "x = # --fill here-- # Conv2D\n",
        "x = # --fill here-- # UpSampling\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixars3zYdDVG",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsCGqhBldDVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr-by0-4dDVM",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CQzVnquCdDVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM0qbl0LdDVQ",
        "colab_type": "text"
      },
      "source": [
        "#### Plot training & validation loss values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ORSz29e7dDVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN5L8iOXdDVT",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize some reconstructed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgaheHI-dDVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgzR3VrAdDVV",
        "colab_type": "text"
      },
      "source": [
        "Two interesting practical applications of autoencoders are data denoising and dimensionality reduction for data visualization.<br>\n",
        "In this part of the lab we see the application to image denoising."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxVS-hCxdDVV",
        "colab_type": "text"
      },
      "source": [
        "### Image Denoising with Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CLRj7nadDVW",
        "colab_type": "text"
      },
      "source": [
        "#### Create noisy samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra23CtHSdDVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have just done the previous part (Convolutional Autoencoder), you've just load the dataset;\n",
        "# otherwise you have to load data \n",
        "\n",
        "noise_factor = # --fill here-- #\n",
        "x_train_noisy = # --fill here-- # add noise to x_train\n",
        "x_test_noisy = # --fill here-- # add noise to x_test\n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.) # required to put all the values in [0,1]\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sS6Sp3HdDVb",
        "colab_type": "text"
      },
      "source": [
        "#### Show some noisy samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcrYP4i-dDVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(1,n):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnC2VIgwdDVf",
        "colab_type": "text"
      },
      "source": [
        "#### Build a Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9sJZoYhdDVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = # --fill here-- #\n",
        "x = # --fill here-- #\n",
        "x = # --fill here-- #\n",
        "x = # --fill here-- #\n",
        "encoded = # --fill here-- #\n",
        "\n",
        "# at this point the representation is (7, 7, 32)\n",
        "\n",
        "x = # --fill here-- #\n",
        "x = # --fill here-- #\n",
        "x = # --fill here-- #\n",
        "x = # --fill here-- #\n",
        "decoded = # --fill here-- #\n",
        "\n",
        "autoencoder = # --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJokaYy8dDVj",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gwQXy-6dDVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RezbnTidDVn",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rtpHe7IxdDVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVEHYKGgdDVp",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize reconstructed (denoised) images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWXKiFardDVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --fill here-- #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax2gCaHZdDVt",
        "colab_type": "text"
      },
      "source": [
        "#### Try to play with the noise_factor value: is there a threshold from which is not possible to reconstruct the image?\n",
        "#### How does the increase of the noise_factor value affect the number of epochs required in the training process?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtmwflIfdDWb",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Generative Adversarial Model\n",
        "\n",
        "GAN is a framework in which two models are simultaneously trained:\n",
        "     - a generative model G that captures the data distribution\n",
        "     - a discriminative model D that estimates the probability tha a sample came from the training data rather than G.\n",
        "<br>\n",
        "    \n",
        "<img src=\"http://mlclass.epizy.com/lab3_images_notebook/gan.png\" width=700px><br><br>\n",
        "\n",
        "\n",
        "$G$ is pitted against an adversary, $D$, that learns to determine whether a sample is from the model distribution or the data distribution.<br>\n",
        "$G$  generates samples by passing random noise through a MLP; $D$ is also a MLP so we can train both models using only backpropagation and dropout algorithms and samples from $G$ using only forward propagation.<br>\n",
        "\n",
        "Schematically, the GAN looks like this:\n",
        "    1. A generator network maps vectors of shape (latent_dim,) to images of shape (32, 32, 3).\n",
        "    2. A discriminator network maps images of shape (32, 32, 3) to a binary score estimating the probability that the image is real.\n",
        "    3. A gan network chains the generator and the discriminator together: gan(x)=discriminator(generator(x)). Thus this gan         network maps latent space vectors to the discriminator’s assessment of the realism of these latent vectors as decoded by       the generator.\n",
        "    4. You train the discriminator using examples of real and fake images along with “real”/“fake” labels, just as you train       any regular image-classification model.\n",
        "    5 To train the generator, you use the gradients of the generator’s weights with regard to the loss of the gan model. \n",
        "    This means, at every step, you move the weights of the generator in a direction that makes the discriminator more likely to classify as “real” the images decoded by the generator (you train the generator to fool the discriminator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFM9dQr5dDWb",
        "colab_type": "text"
      },
      "source": [
        "#### Load cifar10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7CT9RpydDWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (_, _) = tf.keras.datasets.cifar10.load_data() \n",
        "\n",
        "# select only one class (defined as 0 airplane in example)\n",
        "x_train = x_train[y_train.flatten() == 0] \n",
        "# 0 airplane, 1 automobile, 2 bird, 3 cat, 4 deer, 5 dog, 6 frog, 7 horse, 8 ship, 9 truck\n",
        "\n",
        "#normalize\n",
        "x_train = x_train.astype('float32') / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeGYnSOgdDWe",
        "colab_type": "text"
      },
      "source": [
        "#### Set the network parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZxGsIgldDWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 32\n",
        "height = x_train.shape[1]\n",
        "width = x_train.shape[2]\n",
        "channels = x_train.shape[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhxqkJ4jdDWg",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Generator\n",
        "Instead of max pooling, is recommended to use strided convolutions for downsampling, and LeakyReLU layer instead of a ReLU activation.<br>\n",
        "It’s similar to ReLU, but it relaxes sparsity constraints by allowing small negative activation values.<br><br>\n",
        "<img src=\"http://mlclass.epizy.com/lab3_images_notebook/leakyrelu.jpeg\" width=600px>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDFYRt4qdDWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_input = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "# Transforms the input into a 16 × 16 128-channel feature map\n",
        "x = tf.keras.layers.Dense(128 * 16 * 16)(generator_input)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "x = tf.keras.layers.Reshape((16, 16, 128))(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "# Upsamples to 32 × 32\n",
        "x = tf.keras.layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(256, 5, padding='same')(x) \n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "x = tf.keras.layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = tf.keras.layers.LeakyReLU()(x) \n",
        "x = tf.keras.layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
        "\n",
        "# Instantiates the generator model, which maps the input of shape (latent_dim,) into an image of shape (32, 32, 3)\n",
        "generator = tf.keras.models.Model(generator_input, x) \n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrXphPHcdDWj",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVFbU627dDWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator_input = tf.keras.layers.Input(shape=(height, width, channels))\n",
        "\n",
        "x = tf.keras.layers.Conv2D(128, 3)(discriminator_input)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "x = tf.keras.layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "x = tf.keras.layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "x = tf.keras.layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "# Classification layer\n",
        "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Instantiates the discriminator model, which turns a (32, 32, 3) input into a binary classification decision (fake/real)\n",
        "discriminator = tf.keras.models.Model(discriminator_input, x)\n",
        "discriminator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoHTlNYZdDWl",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBZblSyFdDWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another suggestion is lr =0.0005 if the discrimnator is prevailing\n",
        "discriminator_optimizer = tf.keras.optimizers.RMSprop(lr=0.0008,clipvalue=1.0,decay=1e-8)\n",
        "\n",
        "# --fill here-- # compile the descriminator: loss to be used is binary crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcRgWN6AdDWo",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Adversarial Network (chains the generator and the discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys-00CKhdDWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sets discriminator weights to non-trainable (this will only apply to the gan model)\n",
        "discriminator.trainable = False \n",
        "\n",
        "gan_input = tf.keras.layers.Input(shape=(latent_dim,)) \n",
        "gan_output = discriminator(generator(gan_input))\n",
        "\n",
        "gan = tf.keras.models.Model(gan_input, gan_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx8VMtPndDWs",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR0LaKfZdDWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_optimizer = tf.keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0,decay=1e-8)\n",
        "\n",
        "# --fill here-- # compile the gan: loss to be used is binary crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4GLkAjydDWw",
        "colab_type": "text"
      },
      "source": [
        "#### Train the gan \n",
        "Schematically, the training process looks like: <br>\n",
        "    - For each epoch:\n",
        "        1. Draw random points in the latent space (random noise)\n",
        "        2. Generate images with generator using this random noise\n",
        "        3. Mix the generated images with real ones\n",
        "        4. Train discriminator using these mixed images (“real”/“fake” (generated by the generator))\n",
        "        5. Draw new random points in the latent space\n",
        "        6. Train gan using these random vectors, with targets that all say “these are real images.” This updates the weights of the generator (only, because the discriminator is frozen inside gan) to move them toward getting the discriminator to predict “these are real images” for generated images: this trains the generator to fool the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4nVD2yZKdDWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!!! PREDEFINED ONLY 100 ITERATIONS, BUT WILL NEED MORE FOR DECENT RESULTS !!!#\n",
        "iterations = 100\n",
        "batch_size = 20\n",
        "save_dir = 'generated_images'\n",
        "start = 0\n",
        "\n",
        "for step in range(iterations):\n",
        "    \n",
        "    # 1. sample random point in the latent space\n",
        "    random_latent_vectors = # --fill here-- # use np.random.normal\n",
        "    \n",
        "    # 2. generate images with generator\n",
        "    generated_images = generator.predict(random_latent_vectors)\n",
        "    \n",
        "    # 3. mix them with real images\n",
        "    stop = start + batch_size\n",
        "    real_images = x_train[start: stop]\n",
        "    combined_images = # --fill here-- # use np.concatenate\n",
        "    \n",
        "    # assembles labels, discriminating real from fake images\n",
        "    labels = np.concatenate([np.ones((batch_size, 1)),np.zeros((batch_size, 1))])\n",
        "    \n",
        "    # adds random noise to the labels\n",
        "    labels += 0.05 * np.random.random(labels.shape)\n",
        "    \n",
        "    # 4. train the discriminator\n",
        "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "    \n",
        "    # 5. draw new random points in the latent space\n",
        "    random_latent_vectors = # --fill here-- #\n",
        "    \n",
        "    # assembles latent space labels that say “these are all real images” (all zeros)\n",
        "    misleading_targets = # --fill here-- #\n",
        "    \n",
        "    # 6. trains the generator (via the gan model, where the discriminator weights are frozen)\n",
        "    # train_on_batch methods runs a single gradient update on a single batch of data.\n",
        "    a_loss = gan.train_on_batch(random_latent_vectors,misleading_targets)\n",
        "    \n",
        "    start += batch_size\n",
        "    if start > len(x_train) - batch_size:\n",
        "        start = 0\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        gan.save_weights('gan.h5')\n",
        "        print('discriminator loss:%s; adversarial loss:%s'%(d_loss, a_loss))\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "        imgG = image.array_to_img(generated_images[0] * 255., scale=False)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(imgG)\n",
        "        #img.save(os.path.join(save_dir,'generated' + str(step) + '.png'))\n",
        "        imgR = image.array_to_img(real_images[0] * 255., scale=False)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(imgR)\n",
        "        plt.show()\n",
        "        #img.save(os.path.join(save_dir,'real' + str(step) + '.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0LKOcYPdDW3",
        "colab_type": "text"
      },
      "source": [
        "When training, you may see the adversarial loss begin to increase considerably, while the discriminative loss tends to zero so the discriminator may end up dominating the generator.<br>\n",
        "If that’s the case, try reducing the discriminator learning rate, and increase the dropout rate of the discriminator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9kr9wsdDW4",
        "colab_type": "text"
      },
      "source": [
        "#### Plot some examples from training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeHHG1uUdDW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "for i in range(36):\n",
        "    plt.subplot(6, 6, i + 1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image.array_to_img(x_train[i] * 255., scale=False))\n",
        "\n",
        "plt.subplots_adjust(hspace=0, wspace=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g0qEsZMdDW6",
        "colab_type": "text"
      },
      "source": [
        "#### Plot some generated images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvXxfJcgdDW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "for i in range(36):\n",
        "    plt.subplot(6, 6, i + 1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image.array_to_img(generated_images[i] * 255., scale=False), aspect='auto')\n",
        "\n",
        "plt.subplots_adjust(hspace=0, wspace=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}